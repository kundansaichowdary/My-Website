<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DL Project</title>

  <!--
    - favicon
  -->
  <link rel="shortcut icon" href="./assets/images/logo.ico" type="image/x-icon">

  <!--
    - custom css link
  -->
  <link rel="stylesheet" href="./assets/css/style.css">

  <!--
    - google font link
  -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">

  <style>

    .main {
    margin: 15px 12px;
    margin-bottom: 75px;
    min-width: 259px;
}

    Projectmain {
    max-width: 1200px;
    margin-inline: auto;
    display: flex;
    justify-content: center;
    align-items: stretch;
    gap: 25px;
  }

  .Projectmain-content {
    min-width: 105%;
    width: 100%;
  }
  #PT{
    text-align:center;
    margin-top: 20px;
    margin-bottom: 0px ;
  }
  .proImg{
    margin: 30px 150px 10px 30px;
    border-radius: 10px;
    height: 350px;
    width: 750px;
  }

  .exImg{
    margin: 30px 10px 10px 0px;
    border-radius: 10px;
    height: 1200px;
    width: 850px;
  }
  .fImg{
    margin: 30px 10px 10px 0px;
    border-radius: 10px;
    height: 200px;
    width: 800px;
  }
  .resImg{
    margin: 30px 150px 10px 30px;
    border-radius: 10px;
    height: 750px;
    width: 750px;
  }
  .eqnImg{
    margin: 30px 150px 10px 30px;
    border-radius: 10px;
    height: 200px;
    width: 750px;    
  }
  </style>
</head>

<body>

    <header>
        <h1 class="h2" id="PT">Advanced Banking Customer Churn Prediction</h1>
      </header>

  <main>



      <div class="sidebar-info">


        <!-- <div class="info-content">
          <h1 class="name" title="Kc">Project Title yet to be decided</h1>

          
          <p></p>

          <p class="title">     Machine Learning Project  </p>
        </div>
       -->
        <button class="info_more-btn" data-sidebar-btn>
          <span>Show Contacts</span>

          <ion-icon name="chevron-down"></ion-icon>
        </button>

       

       

</div>



      <!--
        - HAVE TO CHANGE LINKS
      -->




<!-- 
 <div class="separator"></div>  -->

    <!--
      - #main-content
    -->
   
    <div class="main-content">


      <!--
        - #NAVBAR
      -->

      <nav class="navbar">

        <ul class="navbar-list">

          <li class="navbar-item">
            <button class="navbar-link  active" data-nav-link>Introduction</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>DataPrep_EDA</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Algorithms</button>
          </li>

          <!-- <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Certifications</button>
          </li> -->

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Conclusion</button>
          </li>

        </ul>

      </nav>






      <!--
        - #ABOUT
      -->

      <article class="about  active" data-page="introduction">

        <header>
          <h2 class="h2 article-title">Introduction</h2>
        </header>

        <section class="about-text">
          
    <p>
            
          In the fast-paced and competitive landscape of digital banking, mastering customer retention has become a pivotal challenge for financial institutions worldwide. It transcends the traditional measure of success, positioning itself as an essential determinant of an institution's survival and growth. Given the critical nature of this issue, a detailed analysis has been initiated to investigate the underlying causes of customer attrition within the banking sector. The project's ambition is to leverage advanced machine learning , Deep Learning and data analytics techniques not only to predict the likelihood of customers leaving their bank but also to provide actionable insights that can inform the development of effective retention strategies.
<br>
<br>
          The inception of this project was motivated by the significant hurdles faced by financial entities in adapting to the digital era, with a particular focus on improving customer loyalty and reducing churn rates. To build a robust analytical framework, insights were gathered from a wide array of sources, including professionals with firsthand banking experience and experts dedicated to the study of customer churn analytics. This collective pool of knowledge has significantly shaped the projectâ€™s strategic direction, enabling a more informed and comprehensive approach to addressing customer retention challenges.

          <br>
          <br>
          At the heart of the project lies the application of sophisticated data analysis and predictive modeling techniques. By systematically analyzing customer data, the project aims to identify patterns and predictors of churn, offering banks a predictive lens through which potential attrition can be foreseen and preemptively addressed. This analytical endeavor seeks not just to react to customer churn but to anticipate it, allowing financial institutions to proactively engage with their customers in ways that reinforce loyalty and satisfaction.

          <br>
          <br>
          Beyond mere prediction, the project aspires to translate data-driven insights into practical strategies for customer retention. It emphasizes the development of tailored interventions that can effectively respond to the identified drivers of customer dissatisfaction and attrition. By doing so, the project offers banks a roadmap to not only keep their customers but also to enhance the overall customer experience, thereby fostering a sense of belonging and loyalty among their clientele.
          <br>
          <br>

          Ultimately, this project stands as a testament to the transformative power of data science in redefining how financial institutions approach customer retention in the digital age. By harnessing the insights gleaned from data analytics and machine learning, banks are equipped with the tools necessary to navigate the complexities of customer relationships in the contemporary banking landscape. Through targeted strategies informed by deep analytical insights, financial institutions can achieve a competitive edge, ensuring their growth and resilience in the face of evolving market dynamics.
          <br>
    </p>    
          <img class="proImg" src="projectImages/introImage.png" alt="Intro Image">
          
        </section>

        <header>
          <h2 class="h2 article-title">State of the art</h2>
        </header>

        <section class="about-text">
          <p> In the digital banking sector, the fight against customer churn has been significantly advanced by the integration of sophisticated machine learning technologies. Deep learning, ensemble methods like Random Forests and Gradient Boosting Machines, and clustering techniques such as K-means have become pivotal in uncovering complex customer behavior patterns and segmenting the customer base for tailored retention strategies. These models are complemented by automated feature engineering, which efficiently extracts predictive insights from extensive datasets, and Explainable AI (XAI), which offers a deeper understanding of churn drivers. Real-time analytics further empower banks to act swiftly in mitigating churn risks, facilitating a proactive approach to customer retention. </p> 

          <p> Moreover, the inclusion of external data sources enriches predictive models with broader contextual insights, enhancing accuracy. As banks navigate the intricacies of digital customer engagement, regulatory and ethical considerations, particularly around data privacy and ethical AI usage, remain at the forefront of strategic planning. This dual focus on cutting-edge analytical techniques and ethical compliance underscores the banking industry's commitment to leveraging technological advancements for improved customer loyalty, setting new standards for personalized banking experiences in an increasingly competitive landscape. </p>
         
        </section>

       
        <header>
            <h2 class="h2 article-title">Objective</h2>
          </header>
  
          <section class="about-text">
            <p>  The primary objective is to establish a robust predictive model capable of identifying customers at the highest risk of churning in the near term. The ambition extends beyond simple predictions to a thorough analysis and interpretation of the data to reveal the key drivers of customer churn. These insights are deemed crucial for empowering banks with the knowledge and strategies to not only retain customers but also to foster deeper loyalty and satisfaction. Ultimately, the project is geared towards enabling banks to proactively engage with their customer base, using predictions and insights to formulate and implement effective retention strategies in the competitive domain of digital banking. </p> 
            <img class="proImg" src="projectImages/churnImage.png" alt="Churn Image">
          </section>

  
          <header>
            <h3 class="h3 article-title">Questions that I intend to investigate by the end of the project:</h3>
          </header>
  
          <section class="about-text">
            <p>
     <ol>
      <li>1. What demographic characteristics (age, gender, education level) most significantly influence a customer's decision to leave the bank? </li>
      <li>2. How does marital status and the number of dependents affect customer loyalty and churn rates?</li>
      <li>3. Can we predict customer churn based on account information such as months on book and credit limit?</li>
      <li>4. What impact do behavioral metrics (e.g., months inactive, contact count in the last 12 months) have on predicting customer attrition?</li>
      <li>5. How does the utilization ratio correlate with customer satisfaction and retention?</li>
      <li>6. Can association rule mining uncover common pathways or triggers leading to customer churn?</li>
      <li>7. What are the most significant factors that contribute to a customer becoming inactive or reducing their transaction activity?</li>
      <li>8. Is there a relationship between the change in transaction count from Q4 to Q1 and the likelihood of customer attrition?</li>
      <li>9. How does the use of different card categories (Blue, Silver, Gold, Platinum) relate to customer loyalty and attrition rates?</li>
      <li>10. Can a pattern of transactions (total transaction amount, total transaction count) predict upcoming customer churn?</li>
     </ol>
            
          </section>
          





        <div class="modal-container" data-modal-container>

            <div class="overlay" data-overlay></div>
  
            <section class="testimonials-modal">
  
              <button class="modal-close-btn" data-modal-close-btn>
                <ion-icon name="close-outline"></ion-icon>
              </button>
       </section>
  
          </div>
        
        <!--
          - clients
        -->



        
        



      </article>


        <!--
            - #RESUME
          -->

        <article class="resume" data-page="dataprep_eda">



          <!-- <header>
            <h2 class="h3 article-title">API</h2>
          </header>
          <section class="about-text">
            <p><b>Raw data from Newsapi.org :</b> <a style="display: inline;" target="_blank" href="https://newsapi.org/v2/everything?q=bank+customer+churn+data&apiKey=your_api_key">https://newsapi.org/v2/everything?q=bank+customer+churn+data&apiKey=your_api_key</a>
            </p>
           
            <img class="proImg" src="projectImages/rawApiData.png" alt="Raw Api Data">
            <p>
              In this area of project, the process began with fetching data from NewsAPI, aiming to analyze trends and insights on customer churn and retention strategies from a collection of news articles. The data, initially in JSON format, contained detailed attributes such as the source, author, title, description, publication date, and content of each article.
            </p>
              <p>
              The raw JSON data was then transformed into a pandas DataFrame. This step was critical as it facilitated easier manipulation and analysis of the data, leveraging the capabilities of pandas for efficient data handling and operations.
              </p>
              <p>
                Following the transformation, the data underwent a cleaning and preprocessing phase. Key steps in this process included:
                <ol>
                  <li> 1. Converting the `publishedAt` field from a string format to a datetime object, enabling time-based sorting and analysis.</li>
                  <li> 2. Filling missing values in the `author` column with 'Unknown' to ensure the dataset was complete and consistent, avoiding potential biases in analysis due to missing information.</li>
                  <li> 3. Extracting the `name` attribute from the nested `source` dictionary for each article and placing it in a new `source_name` column. This step flattened the dataset's structure, removing the original `source` column to streamline the data further.</li>
                </ol>
              </p>
              <p><b>Cleaned Api data :</b></p>
              <img class="proImg" src="projectImages/cleanedApiData.png" alt="Cleaned Api Data">

              <p><b>Access to the Code :</b> <a style="display: inline;" target="_blank" href="https://colab.research.google.com/drive/1SaeUmRwAM2lZYH8MlScEAgrFYAQyGXgh?usp=sharing">Colab Link</a>
              </p>
            
          </section> -->
          
          
              <header>
                <h2 class="h3 article-title">Dataset Overview</h2>
              </header>
              <section class="about-text">
                <p>
                  The dataset, sourced from <a style="display: inline-grid;" href="https://leaps.analyttica.com/home">https://leaps.analyttica.com/home</a>, encompasses detailed records of 10,000 bank customers, shedding light on various aspects such as age, salary, marital status, credit card limit, and card category, among others. With close to 20 features, it provides a holistic view of the banking institution's clientele, particularly focusing on credit card users. The primary challenge highlighted by this dataset is the skewed distribution of churned customers, who represent only 16.07% of the total, posing difficulties in training predictive models due to the imbalance.
                </p>

                <p>
                  This dataset encapsulates a comprehensive view of customer profiles within a banking institution, capturing demographic information, financial behaviors, and engagement levels. The variety of features included allows for an in-depth analysis of factors influencing customer loyalty and churn. Leveraging machine learning algorithms, the dataset not only facilitates the prediction of customer attrition but also aids in devising targeted retention strategies. Its structure is conducive to both exploratory data analysis and predictive modeling, making it an invaluable resource for enhancing customer retention efforts in the banking sector.
                </p>
                <img class="proImg" src="projectImages/dataAtt.png" alt="Data Attributes">
                <p style="text-align: center;">Table.1 provides an overview of each variable within the dataset</p>
      
              </section>
             
              <header>
                <h2 class="h3 article-title">Exploratory Data Analysis</h2>
              </header>
              <section class="about-text">
                <p>
                  Exploratory Data Analysis (EDA) plays a crucial role in the data science field, acting as an initial step to uncover patterns, detect anomalies, identify significant variables, and test hypotheses. EDA provides valuable insights that inform the later stages of analysis, enabling data-driven decision-making based on a comprehensive understanding of the dataset's features and relationships. In this project, an in-depth EDA was performed on a banking dataset, focusing on customer demographics such as age, gender, education level, marital status, and income category, alongside banking behavior metrics including credit limits, revolving balances, transaction volumes, and frequencies. Through a variety of visualizations, including histograms, count plots, and density plots, the project effectively visualized data distributions and relationships, undertook data cleaning by imputing missing values, and prepared the dataset for further analysis by encoding categorical variables. This meticulous exploratory process ensures that subsequent predictive modeling and strategic decisions are based on a nuanced and detailed understanding of the underlying data.
                </p>
              </section> 
                
                  <h2 class="h3 service-title">Visualisations</h2>
               

                <section class="about-text">
                  <h4 class="h4 service-title">Visualization 1: Distribution of Customer Age</h4>
                  <img class="proImg" src="projectImages/v1.png" alt="Visualisation-1">
                  <p>
                    Observations:
                    <ol>

                    <li> - <b>Distribution Shape:</b> The 'Customer_Age' histogram appears to be fairly symmetrical, suggesting a normal-like distribution.</li>
                    <li> - <b>Central Tendency:</b> The age of customers clusters around the mid-40s, indicating this is the most common age group in the dataset.</li>
                    <li> - <b>Spread: </b> The age range extends from the mid-20s to early 70s, with a concentration of data points in the 40-55 age range.</li>
                    <li> - <b>Outliers: </b>There do not appear to be any significant outliers, as the distribution tails off smoothly at both ends.</li>
                    <li> - <b>Overall Interpretation: </b>The customer base seems to be middle-aged, with a balanced age distribution. This might reflect a broad appeal of the bank's services across different age groups.</li>
                    </ol> 
                </p>
                </section> 

              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 2: Gender Distribution</h4>
                  <img class="proImg" src="projectImages/v2.png" alt="Visualisation-2">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution:</b> The graph illustrates the gender distribution of customers, categorized as 'M' (Male) and 'F' (Female).</li>
                  <li> - <b>Gender Ratio:</b> There is a slight predominance of female customers compared to male customers in the dataset.</li>
                  <li> - <b>Balance: </b> While not perfectly balanced, the gender distribution is relatively even, indicating a diverse customer base in terms of gender.</li>
                  <li> - <b>Implications for Analysis: </b>The relatively balanced gender distribution is beneficial for analyses that might be sensitive to gender proportions, such as customer behavior studies or targeted marketing strategies.</li>
                  <li> - <b>Overall Interpretation: </b>The distribution suggests that the bankâ€™s customer base is quite diverse in terms of gender, which might reflect the bank's broad appeal across different gender demographics.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 3: Education Level Distribution</h4>
                  <img class="proImg" src="projectImages/v3.png" alt="Visualisation-3">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution:</b> The graph shows a variety of education levels among the bank's customers, with a significant portion having achieved a graduate level education.</li>
                  <li> - <b>Common Education Levels:</b> High School and Graduate are common, indicating a broad range of educational backgrounds.</li>
                  <li> - <b>Variation: </b> Noticeable decrease in frequency beyond the Graduate level, with fewer customers holding post-graduate qualifications.</li>
                  <li> - <b>Overall Interpretation: </b>The diverse educational background of customers may influence their banking needs and preferences.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 4: Marital Status Distribution</h4>
                  <img class="proImg" src="projectImages/v4.png" alt="Visualisation-4">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution:</b> The graph presents the marital status of customers, revealing a mix of single, married, and divorced individuals.</li>
                  <li> - <b>Most Common Status:</b> Married status is the most common, suggesting that a significant portion of the customer base may have joint financial interests or family-oriented financial needs.</li>
                  <li> - <b>Variation: </b> Single and Divorced statuses are less common but represent a notable segment of the customer base.</li>
                  <li> - <b>Overall Interpretation: </b>Understanding the marital status distribution can help the bank tailor its products and services to meet the needs of different life stages.</li>
                  </ol> 
              </p>
              </section>
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 5: Income Category Distribution</h4>
                  <img class="proImg" src="projectImages/v5.png" alt="Visualisation-5">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution:</b> Displays the income categories of customers, indicating the economic diversity within the bank's customer base.</li>
                  <li> - <b>Income Range:</b> Spans from Less than \$40K to \$120K+, with a significant number of customers in the lower income brackets.</li>
                  <li> - <b>Skewness: </b> The distribution is skewed towards lower income categories, suggesting the bank serves a wide range of economic demographics.</li>
                  <li> - <b>Overall Interpretation: </b> The income distribution provides insights into the financial positioning of customers, which can inform targeted marketing and service development.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 6: Card Category Distribution</h4>
                  <img class="proImg" src="projectImages/v6.png" alt="Visualisation-6">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution:</b> Highlights the types of cards customers use, such as Blue, Silver, Gold, and Platinum.</li>
                  <li> - <b>Card Preferences:</b> A vast majority opt for the Blue card, with progressively fewer customers using the more premium card types.</li>
                  <li> - <b>Skewness: </b> The distribution is highly skewed towards the Blue card, indicating a preference for basic card services.</li>
                  <li> - <b>Overall Interpretation: </b> The bank's customer base predominantly prefers the Blue card, which may suggest a focus on essential card services or a potential opportunity to promote premium cards.</li>
                  </ol> 
              </p>
              </section>
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 7: Months on Book Distribution</h4>
                  <img class="proImg" src="projectImages/v7.png" alt="Visualisation-7">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The histogram shows a bell-shaped distribution centered around a medium-term relationship duration with the bank.</li>
                  <li> - <b>Central Tendency:</b> The aggregation of data points around the middle of the range indicates that the average customer has been with the bank for a moderate duration.</li>
                  <li> - <b>Spread: </b> The distribution spans a broad range, from new clients to those with longstanding relationships.</li>
                  <li> - <b>Outliers: </b> There are few to no extreme values, suggesting that customers do not significantly deviate from the average relationship duration.</li>
                  <li> - <b>Overall Interpretation: </b> The bank has a stable customer base, with most customers maintaining their relationship for a similar period, possibly reflecting satisfaction with the bank's services.</li>
                  </ol> 
              </p>
              </section>
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 8: Total Relationship Count Distribution</h4>
                  <img class="proImg" src="projectImages/v8.png" alt="Visualisation-8">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The count plot reveals a high frequency of customers with three to four product relationships with the bank.</li>
                  <li> - <b>Central Tendency:</b> The most common relationship counts suggest that the average customer engages with multiple services.</li>
                  <li> - <b>Spread: </b>  A decreasing frequency for higher relationship counts may indicate fewer cross-selling successes for additional products.</li>
                  <li> - <b>Outliers: </b> There are no apparent outliers, reflecting a standard customer engagement level.</li>
                  <li> - <b>Overall Interpretation: </b> The bankâ€™s customers tend to engage with several products, indicating opportunities for increasing product penetration and cross-selling.</li>
                  </ol> 
              </p>
              </section>
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 9: Attrition Flag Distribution</h4>
                  <img class="proImg" src="projectImages/v9.png" alt="Visualisation-9">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The plot shows a large disparity between existing and attrited customers, with existing customers far outnumbering those who have left.</li>
                  <li> - <b>Central Tendency:</b> The high count of existing customers suggests successful customer retention strategies.</li>
                  <li> - <b>Spread: </b>  The low number of attrited customers indicates that customer churn is relatively uncommon.</li>
                  <li> - <b>Outliers: </b> The distribution lacks significant outliers, suggesting a stable customer base.</li>
                  <li> - <b>Overall Interpretation: </b> The bank has been effective in maintaining its customer base, although the presence of attrited customers points to areas where retention efforts can be improved.</li>
                  </ol> 
              </p>
              </section>
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 10: Credit Limit Distribution</h4>
                  <img class="proImg" src="projectImages/v10.png" alt="Visualisation-10">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The distribution of 'Credit_Limit' is right-skewed, indicating that a majority of customers have lower credit limits, while a smaller number have significantly higher limits.</li>
                  <li> - <b>Central Tendency:</b> The mean credit limit is higher than the median, which is pulled up by high-value outliers.</li>
                  <li> - <b>Spread: </b>  The credit limits range broadly from low to very high values, suggesting diverse financial standings among customers.</li>
                  <li> - <b>Outliers: </b> High-value outliers are present, indicating some customers with exceptionally high credit limits.</li>
                  <li> - <b>Overall Interpretation: </b> The bank serves customers with varying credit needs, from those with modest credit requirements to those with substantial credit lines.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 11: Total Revolving Bal Distribution</h4>
                  <img class="proImg" src="projectImages/v11.png" alt="Visualisation-11">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The 'Total_Revolving_Bal' has a multimodal distribution, with several peaks indicating common balance levels.</li>
                  <li> - <b>Central Tendency:</b> The presence of multiple peaks affects the mean and median, which may not accurately represent the most common balance levels.</li>
                  <li> - <b>Spread: </b> The balances range widely, with many customers maintaining low revolving balances.</li>
                  <li> - <b>Outliers: </b> Some customers have very high revolving balances, but they are not excessively distant from the rest of the data.</li>
                  <li> - <b>Overall Interpretation: </b> Customers exhibit diverse usage of revolving credit, with a significant number either paying off their balances or not using their credit regularly.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 12: Avg Open To Buy Distribution</h4>
                  <img class="proImg" src="projectImages/v12.png" alt="Visualisation-12">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The 'Avg_Open_To_Buy' distribution is right-skewed, with many customers having lower amounts of unused credit.</li>
                  <li> - <b>Central Tendency:</b> The mean is notably higher than the median, reflecting the skewness of the distribution.</li>
                  <li> - <b>Spread: </b> There is a wide range of 'Avg_Open_To_Buy' values, with some customers having very high amounts of available credit.</li>
                  <li> - <b>Outliers: </b> Customers with exceptionally high 'Avg_Open_To_Buy' values are present, likely due to high credit limits or conservative credit use.</li>
                  <li> - <b>Overall Interpretation: </b> Most customers maintain lower levels of unused credit, but a segment has significantly higher credit availability.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 13: Total Transaction Amount Distribution</h4>
                  <img class="proImg" src="projectImages/v13.png" alt="Visualisation-13">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The 'Total_Trans_Amt' distribution is right-skewed, indicating that most customers have lower transaction amounts.</li>
                  <li> - <b>Central Tendency:</b> The mean transaction amount is higher than the median, influenced by customers with higher transaction volumes.</li>
                  <li> - <b>Spread: </b> The transaction amounts vary widely, with some customers having very high total transaction amounts.</li>
                  <li> - <b>Outliers: </b> There are outliers with very high transaction amounts, potentially reflecting a small group of customers with high spending.</li>
                  <li> - <b>Overall Interpretation: </b> While most customers have modest transaction volumes, a subset engages in more significant spending.</li>
                  </ol> 
              </p>
              </section>
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 14: Total Transaction Count Distribution</h4>
                  <img class="proImg" src="projectImages/v14.png" alt="Visualisation-14">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The 'Total_Trans_Ct' distribution is somewhat bimodal, indicating common transaction count ranges.</li>
                  <li> - <b>Central Tendency:</b> The mean and median are closely aligned, suggesting a balanced distribution around the central transaction counts.</li>
                  <li> - <b>Spread: </b> The transaction counts are varied, reflecting diverse transaction behaviors among customers.</li>
                  <li> - <b>Outliers: </b> There are no significant outliers; most transaction counts fall within a typical range.</li>
                  <li> - <b>Overall Interpretation: </b> The distribution points to two distinct customer groups based on transaction frequency, indicating different patterns of bank usage.</li>
                  </ol> 
              </p>
              </section>
              
              
              <section class="about-text">
                  <h4 class="h4 service-title">Visualization 15: Avg Utilization Ratio Distribution</h4>
                  <img class="proImg" src="projectImages/v15.png" alt="Visualisation-15">
                  <p>
                  Observations:
                  <ol>
              
                  <li> - <b>Distribution Shape:</b> The 'Avg_Utilization_Ratio' distribution is bimodal, with peaks at lower and higher utilization ratios.</li>
                  <li> - <b>Central Tendency:</b> Both the mean and median are influenced by the two distinct peaks in the distribution.</li>
                  <li> - <b>Spread: </b> The utilization ratio ranges from 0 to just under 1.0, highlighting varying credit use levels among customers.</li>
                  <li> - <b>Outliers: </b> No significant outliers are observed, with most values within a standard range for utilization ratios.</li>
                  <li> - <b>Overall Interpretation: </b> The customer base is divided into two - segments: those who use a low proportion of their credit and those with higher utilization, indicating different credit management behaviors.</li>
                  </ol> 
              </p>
              </section>
                
            </section>

            <section class="about-text">
                <h4 class="h4 service-title">Visualization 16: Heatmap of Correlation Matrix</h4>
                <img class="proImg" src="projectImages/v16.png" alt="Visualisation-16">
                <p>
                Observations:
                <ol>
            
                    <li> - <b>Visualization:</b> The heatmap offers a comprehensive view of the correlations between all numerical variables in the dataset. The color intensity represents the strength and direction of the correlation, with warmer colors indicating positive correlations and cooler colors showing negative correlations.</li>
                    <li> - <b>Insight: </b> This allows us to quickly identify which pairs of variables are most strongly correlated, aiding in the identification of potential relationships or redundancies within the dataset.</li>
                </ol> 
            </p>
            </section>
            
            <section class="about-text">
                <h4 class="h4 service-title">Visualization 17: Total Trans Amount by Age Group </h4>
                <img class="proImg" src="projectImages/v17.png" alt="Visualisation-17">
                <p>
                Observations:
                <ol>
            
                <li> - <b>Visualization:</b> The violin plot also categorizes customers by age and visualizes the distribution of total transaction amounts for each group, combining aspects of box plots with a kernel density estimation.</li>
                <li> - <b>Insight: </b> This provides a more nuanced view of the total transaction amounts across age groups, showing not only the central tendency and variability but also the distribution shape. It can reveal, for example, if certain age groups have more uniform spending patterns or if there are subgroups within the ages that behave differently.</li>
                </ol> 
            </p>
            </section>
            
            <section class="about-text">
                <h4 class="h4 service-title">Visualization 18: Customer Age and Months on Book </h4>
                <img class="proImg" src="projectImages/v18.png" alt="Visualisation-18">
                <p>
                Observations:
                <ol>
            
                <li> - <b>Visualization:</b> A scatterplot showcasing the relationship between 'Customer Age' and 'Months on Book', highlighted in royal blue for easy visualization.</li>
                <li> - <b>Insight: </b> Demonstrates a positive linear correlation, indicating older customers tend to have longer relationships with the bank. This suggests loyalty increases with age, providing valuable information for customer retention strategies.</li>
                </ol> 
            </p>
            </section>
            
            <section class="about-text">
                <h4 class="h4 service-title">Visualization 19: Credit Limit and Avg Open To Buy </h4>
                <img class="proImg" src="projectImages/v19.png" alt="Visualisation-19">
                <p>
                Observations:
                <ol>
            
                <li> - <b>Visualization:</b> Scatterplot displaying the relationship between 'Credit Limit' and 'Avg Open To Buy', marked in vibrant orange.</li>
                <li> - <b>Insight: </b> Reveals a strong positive correlation, indicating that customers with higher credit limits generally have more available credit. This relationship underscores the financial behavior of customers and can guide credit management policies.</li>
                </ol> 
            </p>
            </section>
            
            <section class="about-text">
                <h4 class="h4 service-title">Visualization 20: Total Trans Amount and Total Trans Ct </h4>
                <img class="proImg" src="projectImages/v20.png" alt="Visualisation-20">
                <p>
                Observations:
                <ol>
            
                <li> - <b>Visualization:</b> A scatterplot illustrating the relationship between 'Total Transaction Count' and 'Total Transaction Amount', in a distinct sea green.</li>
                <li> - <b>Insight: </b> Shows a very strong positive correlation, suggesting that an increase in the number of transactions correlates with an increase in the total amount spent. This insight is crucial for identifying highly engaged customers and tailoring marketing strategies to boost transaction activities.</li>
                </ol> 
            </p>
            </section>

                
              
              <header>
                <h2 class="h3 article-title">Data Preprocessing and Cleaning</h2>
              </header>
              <section class="about-text">
                <p>
                  During the data preprocessing phase for a project focused on predicting customer churn in the banking sector, the dataset presented unique challenges, particularly in handling categorical data with 'Unknown' placeholders. A significant portion of the data for attributes like Education_Level, Marital_Status, and Income_Category contained 'Unknown' values, posing a potential risk to the integrity and reliability of subsequent analyses. To address this, several strategies were employed:
                </p>

                <p>                
                 <b>1. Data Cleaning and Placeholder Imputation:</b>  The primary approach was to replace 'Unknown' placeholders with the mode of each categorical column, a widely accepted practice that preserves dataset integrity without introducing substantial bias. Despite considering model imputation using a Decision Tree algorithm, this method was ultimately abandoned due to subpar results, attributed to the dataset's limitations or the intricate interactions between variables. Excluding records with 'Unknown' values was also deemed inappropriate due to the dataset's constrained size, which could have led to biased outcomes.
                </p>
                <img class="proImg" src="projectImages/phCount.jpg" alt="Placeholder Count">
                <p>
                  <b>2. Feature Encoding:</b> To prepare the dataset for machine learning algorithms, encoding techniques were applied to transform categorical data into a machine-readable format. One Hot Encoding was utilized for nominal data without an inherent order, such as Gender, Marital_Status, and Card_Category, creating binary columns for each category within a feature. Conversely, Label Encoding was employed for ordinal data, where categories possess a natural order, assigning integers that preserve this hierarchy to variables like Education_Level and Income_Category.
                </p>

                <p>
                  <b>3. Handling Imbalanced Data with SMOTE:</b> The dataset exhibited a pronounced class imbalance in the Attrition_Flag column, with a significant majority of customers classified as "Existing Customer" compared to "Attrited Customer". To counteract this imbalance and enhance model accuracy, the Synthetic Minority Over-sampling Technique (SMOTE) was implemented. SMOTE generated synthetic samples for the minority class, balancing the class distribution and enabling a more equitable and effective model training process. This intervention was crucial for mitigating the inherent bias towards the majority class and improving the model's predictive performance regarding customer attrition.
                </p>
                <img class="proImg" src="projectImages/smot.jpg" alt="SMOT">
                <p>
                  These preprocessing steps, from meticulous data cleaning to strategic feature encoding and addressing class imbalance, laid a solid foundation for developing robust predictive models. By ensuring the dataset was clean, well-structured, and balanced, the project set the stage for insightful analyses and effective churn prediction, ultimately enabling targeted customer retention strategies.
                </p>
         
              </section>

              <section class="about-text">

              <p><b> Intial data : </b></p>

              <img class="proImg" src="projectImages/initData.png" alt="Initial Data">

              <p><b> Cleaned data : </b></p>

              <img class="proImg" src="projectImages/cleanData.png" alt="Cleaned Data">

              <p><b> Final data : </b></p>

              <img class="proImg" src="projectImages/finalData.png" alt="Final Data">
              
              <p><b>Access to the Code :</b> <a style="display: inline;" target="_blank" href="https://colab.research.google.com/drive/1dtaaFEjfRIeZyOxG0sOV6lrmiFHR_CD9?usp=sharing">Colab Link</a>
              </p>
            </section>
              


      </article>





      <!--
        - #PORTFOLIO
      -->



      <article class="portfolio" data-page="algorithms">

     



        <header>
          <h3 class="h3 article-title">DL Architectures</h3>
        </header>

        <section class="projects">

          <ul class="filter-list">

            <li class="filter-item">
              <button class="active" data-filter-btn>All</button>
            </li>

            <!-- <li class="filter-item">
              <button data-filter-btn>Clustering</button>
            </li>


            <li class="filter-item">
              <button data-filter-btn>Association Rule Mining</button>
            </li>

            <li class="filter-item">
              <button data-filter-btn>Naive Bayes</button>
            </li>

            <li class="filter-item">
              <button data-filter-btn>Decision Trees</button>
            </li>

            <li class="filter-item">
              <button data-filter-btn>Support Vector Machines (SVM)</button>  
            </li> -->

            <li class="filter-item">
              <button data-filter-btn>FCNN</button>  
            </li>

            <li class="filter-item">
              <button data-filter-btn>Wide - Deep Models</button>  
            </li>
            
            <li class="filter-item">
              <button data-filter-btn>TabNet</button>  
            </li>

          </ul>

          <div class="filter-select-box">

            <button class="filter-select" data-select>

              <div class="select-value" data-selecct-value>Select category</div>

              <div class="select-icon">
                <ion-icon name="chevron-down"></ion-icon>
              </div>

            </button>

            <ul class="select-list">

              <li class="select-item">
                <button data-select-item>All</button>
              </li>
<!-- 
              <li class="select-item">
                <button data-select-item><id="test">Clustering</button>
              </li>

              <li class="select-item">
                <button data-select-item><id="test">Association Rule Mining</button>
              </li>
              <li class="select-item">
                <button data-select-item><id="test">Naive Bayes</button>
              </li>

              <li class="select-item">
                <button data-select-item>Decision Trees</button>
              </li>

              <li class="select-item">
                <button data-select-item>Support Vector Machines (SVM)</button>
              </li> -->

              <li class="select-item">
                <button data-select-item>FCNN</button>
              </li>

              <li class="select-item">
                <button data-select-item>Wide & Deep Models</button>
              </li>

              <li class="select-item">
                <button data-select-item>TabNet</button>
              </li>


            </ul>

          </div>

          <ul class="resume">

            <!-- <li class="project-item  active" data-filter-item data-category="all">
              <header>
                  <h2 class="h2 article-title">Reason for Choosing these algorithms</h2>
                </header>
        
                <section class="about-text">
                  <p>
                    The selection of Decision Trees, Support Vector Machines (SVM), and K-means Clustering as the primary algorithms for predicting customer churn within the banking sector is strategically motivated by their collective strengths in interpretability, predictive accuracy, and customer segmentation. Decision Trees are particularly valued for their straightforward interpretability and capacity to process both categorical and numerical data efficiently, thereby illuminating key drivers of customer churn. This feature is crucial for identifying actionable intervention points. SVMs are chosen for their exemplary performance in classification challenges, adept at managing the complexity and high dimensionality typical of banking data. Their ability to delineate clear boundaries between churning and retained customers ensures a high level of predictive precision. K-means Clustering complements these predictive models by enabling the identification of inherent customer groupings within the data, based on shared characteristics. This segmentation is instrumental in crafting targeted retention strategies, enhancing the specificity and effectiveness of initiatives aimed at mitigating churn. Collectively, these algorithms form a robust analytical framework capable of not only accurately forecasting churn but also generating deep insights to inform tailored customer retention strategies, thereby addressing churn proactively in a competitive banking environment.
                  </p>
        
                  
                </section>
          </li>   -->

            <li class="project-item  active" data-filter-item data-category="all">
                <header>
                    <h2 class="h2 article-title">Reason for choosing these architectures</h2>
                  </header>
          
                  <section class="about-text">
                    <p>
                      The selection of Fully Connected Neural Networks (FCNN), Wide & Deep Learning, and TabNet for modeling tabular data is strategically motivated by their ability to balance predictive power, scalability, and interpretability, making them well-suited for structured datasets. <br> <br>
                      FCNNs are foundational to this framework, offering the ability to model non-linear relationships between features while serving as a benchmark for performance comparison. Their straightforward architecture ensures adaptability across diverse datasets while highlighting key areas for improvement in subsequent models. <br> <br>
                      The Wide & Deep Model combines the strengths of memorization and generalization. Its wide component explicitly captures feature interactions critical for structured data, while the deep component learns non-linear, hierarchical patterns. This hybrid approach is particularly effective for tasks requiring both explicit feature engineering and deeper pattern extraction. <br> <br>
                      TabNet was chosen as a cutting-edge solution designed explicitly for tabular data. Its sequential attention mechanism dynamically selects the most relevant features for each data point, enabling interpretability without sacrificing performance. Unlike traditional methods, TabNet integrates feature selection into its training process, eliminating the need for manual feature engineering and yielding state-of-the-art results. <br> <br>
                      Together, these algorithms create a comprehensive framework capable of robust predictions, insights into feature importance, and adaptive learning tailored to the nuances of tabular data. This selection reflects a deliberate balance between theoretical rigor and practical application, ensuring high performance and actionable insights across diverse audiences. <br> <br>
                    
                    </p>
          
                    
                  </section>
            </li>   

            <!-- <li class="project-item  active" data-filter-item data-category="naive bayes">
              <header>
                  <h3 class="h3 article-title">NaÃ¯ve Bayes</h3>
                </header>
        
                <header>
                  <h4 class="h4 article-title">Overview</h4>
                </header>

                <section class="about-text">
                    
                  <p>
                    <b>What is NaÃ¯ve Bayes?</b>
                  </p>
                  <p>

                      The NaÃ¯ve Bayes classifier operates as a probabilistic algorithm within the machine learning paradigm, employing Bayesâ€™ theorem under the pivotal yet simplistic assumption of conditional independence among all feature pairs, contingent upon the class variable's value. This algorithm's elegance lies in its straightforwardness, which belies its potential for achieving remarkable accuracy, especially in the analysis of extensive datasets. <br>
                      <br>
                      NaÃ¯ve Bayes encompasses a suite of algorithms characterized by their reliance on Bayesâ€™ theorem, predicated on the notion of mutual independence between predictors. This core principle dictates that the presence (or absence) of a particular feature is assumed to have no bearing on the presence (or absence) of any other feature, given the outcome category. Each feature is thus considered to contribute equally and independently to the probability of the outcome, allowing for the simplification of complex probabilistic modeling. This theoretical framework, despite its inherent assumptions, endows NaÃ¯ve Bayes with a surprising level of efficacy across a myriad of application domains, particularly in scenarios characterized by voluminous datasets. <br>
                  </p>
        
                  <p>
                    <b>NaÃ¯ve Bayes can be broadly categorized into three principal types:</b>
                  </p>
                  <p>
                    <ol>
                      <li>1. Gaussian NaÃ¯ve Bayes </li>
                      <li>2. Multinomial NaÃ¯ve Bayes</li>
                      <li>3. Bernoulli NaÃ¯ve Bayes</li>
                    </ol>
                  </p>
                  <img class="proImg" src="projectImages/naiveBayesIntro.webp" alt="Naive Bayes">
                  <p style="text-align: center;">Simple Representation of the Naive Bayes Classification</p>
                  <p>




<b>Gaussian NaÃ¯ve Bayes:</b> <br>

Gaussian NaÃ¯ve Bayes represents a specialized adaptation of the NaÃ¯ve Bayes framework, tailored for scenarios where the predictor variables exhibit continuous numerical values. This variant hinges on the premise that the continuous attributes associated with each class are governed by a Gaussian (or Normal) distribution, a fundamental concept in probability theory. <br>
<br>
A Gaussian distribution epitomizes a continuous probability distribution for real-valued random variables, characterized by its bell-shaped curve. The probability density function of a Gaussian distribution is defined as: <br>

<img class="eqnImg" src="projectImages/gaussianNaiÌˆveBayes.webp" alt="Gaussian Naive Bayes">

where: <br>
- Î¼ denotes the mean or average of the dataset, providing a centrality measure. <br>
- Ïƒ represents the standard deviation, quantifying the dispersion of the dataset from the mean. <br>
<br>

In applying the Gaussian NaÃ¯ve Bayes algorithm, one computes the mean Î¼ and variance Ïƒ for each class within the training dataset. Predictive modeling involves calculating the Gaussian probability density of each input variable for every class. These probabilities, under the algorithm's 'naÃ¯ve' independence assumption, are then aggregatedâ€”typically through multiplicationâ€”to infer the likelihood of a new instance being associated with each class.
<br>
The algorithm assigns the class with the maximal computed probability as the predictive outcome. Despite the simplifying assumptions regarding data distribution and feature independence, Gaussian NaÃ¯ve Bayes demonstrates remarkable efficacy, particularly in the analysis of high-dimensional datasets, underscoring its utility in a broad spectrum of machine learning applications.
<br>
<br>
<b>Multinomial NaÃ¯ve Bayes:</b> <br>

Multinomial NaÃ¯ve Bayes stands as a specialized extension of the NaÃ¯ve Bayes framework, optimally designed for handling discrete count data. This variant finds extensive application in the realm of text classification, leveraging the frequencies of word occurrences within documents, or the binary presence/absence of specific words, as its defining features. <br>
<br>
The core operational principle of this algorithm is anchored in Bayes' theorem, which it employs to ascertain the likelihood of a given class based on a composite of features, articulated through the formula:
<br>

<img class="eqnImg" src="projectImages/multinomialNaiÌˆveBayes.webp" alt="Multinomial NaÃ¯ve Bayes">
<br>

Here, y represents the target class variable, while (x_1, ..., x_n) constitute the feature vectors. <br>

The essence of this approach lies in the simplification of the joint probability model to the product of the individual feature probabilities conditional on the class, a simplification made possible by the algorithm's foundational assumption of feature independence. <br>
<br>
Training the Multinomial NaÃ¯ve Bayes model entails the determination of the probability of each feature's occurrence within each class label (e.g., distinguishing between spam and non-spam classifications). These probabilities are subsequently employed to predict classifications for new data entries. A notable challenge arises with the introduction of previously unseen feature values (words) in new instances, which, in the absence of smoothing techniques, would result in a zero probability assignment, effectively nullifying the predictive capability. To circumvent this issue, smoothing techniques, most notably Laplace (or add-one) smoothing, are applied to ensure non-zero probability estimates, thereby maintaining each feature's contributory value to the overall classification probability. <br>
<br>
Multinomial NaÃ¯ve Bayes emerges as a robust, efficient, and straightforward algorithm for text classification endeavors. Despite the simplicity of its underlying assumptionsâ€”most notably, the presumption of feature independenceâ€”it frequently delivers strong performance across a spectrum of high-dimensional datasets. The incorporation of smoothing techniques is critical to its functionality, ensuring that no feature is discounted in predictive assessments due to zero probability occurrences. <br>
<br>

<b>Bernoulli NaÃ¯ve Bayes:</b> <br>

Bernoulli NaÃ¯ve Bayes constitutes a distinct adaptation of the NaÃ¯ve Bayes algorithmic family, specifically engineered for binary (or Boolean) feature vectors. These are features restricted to dual outcomes, exemplified by True/False or 1/0 dichotomies, and the model bases its operation on the principles of the Bernoulli distribution. <br>
<br>
Central to the Bernoulli NaÃ¯ve Bayes model is the foundational premise that each feature operates independently of the others, with every individual feature contributing autonomously to the overall class probability. This assumption, emblematic of the 'naÃ¯ve' perspective inherent to all NaÃ¯ve Bayes variants, is particularly critical to the Bernoulli model's functionality. <br>
<br>
Unlike its Multinomial counterpart, which considers the frequency of feature occurrences, Bernoulli NaÃ¯ve Bayes is distinctively designed to factor in the mere presence or absence of a feature as a determinant in class probability assessments. This characteristic renders it exceptionally effective for binary classification tasks, where the analysis is predicated on binary attributes. <br>
<br>
The computational framework for Bernoulli NaÃ¯ve Bayes bears resemblance to that of the Multinomial model, with the notable exception of being tailored to accommodate binary occurrence data. As such, the formula undergoes modifications to align with the binary nature of the input data, ensuring accurate probability estimations under the Bernoulli distribution paradigm. <br>
<br>
Bernoulli NaÃ¯ve Bayes offers a targeted approach for handling binary classification problems, leveraging the simplicity and computational efficiency of the NaÃ¯ve Bayes methodology while addressing the unique challenges posed by binary feature vectors through the adoption of the Bernoulli distribution model. <br>
<br>

Within the context of Bayesian statistics, when presented with a class variable (y) and a dependent feature vector (x_1, x_2, ..., x_n), Bayes' theorem articulates the following fundamental relationship: <br>
<img class="eqnImg" src="projectImages/bernoulliNaiÌˆveBayes1.webp" alt="Bernoulli NaÃ¯ve Bayes"> <br>
<br>
Leveraging the foundational naÃ¯ve independence assumption, which posits that all elements within the feature vector x = {x_1, x_2, ..., x_n} are mutually independent, the complexity of the relationship delineated by Bayes' theorem is markedly simplified, yielding: <br>

<img class="eqnImg" src="projectImages/bernoulliNaiÌˆveBayes2.webp" alt="Bernoulli NaÃ¯ve Bayes"> <br>

In practical applications, attention is primarily focused on the numerator of this probabilistic fraction. This focus arises because the denominator remains invariant with respect to the class variable (y) and, given the fixed values of the feature variables (x_i), it effectively serves as a constant. Thus, the numerator embodies the essence of the joint probability model: <br>
<img class="eqnImg" src="projectImages/bernoulliNaiÌˆveBayes3.webp" alt="Bernoulli NaÃ¯ve Bayes"> <br>

Accordingly, the Bernoulli NaÃ¯ve Bayes classifier synthesizes the prior probabilities of each class, denoted as (P(y)), with the conditional probabilities of each individual feature, P(x_i | y), across the respective classes. This integration is fundamental to the classifier's methodology. <br>
<br>

Despite its conceptual simplicity, the Bernoulli NaÃ¯ve Bayes classifier demonstrates notable efficacy across a variety of contexts, especially in text classification endeavors utilizing binary term occurrence as features. Nonetheless, it is imperative to acknowledge the inherent limitations and assumptions underpinning this model. A thorough comprehension of these facets is indispensable for leveraging Bernoulli NaÃ¯ve Bayes to its fullest potential, cementing its status as an indispensable asset within the arsenal of machine learning methodologies. <br>
<br>

<b>Smoothing and working of bayes theorem </b> <br>

<p> 
 The Necessity of Smoothing in Naive Bayes Models

Smoothing techniques, particularly Laplace smoothing, are essential in Naive Bayes (NB) models to address the problem of zero probabilities. When a feature category or class does not appear in the training set, it would be assigned a probability of zero without smoothing. This situation could drastically skew the model's predictions, as multiplying any probability by zero results in zero, leading to the potential dismissal of valid classifications. Smoothing adjusts for this by adding a small, positive value to the count of each class or feature category, ensuring that no probability is ever exactly zero. This adjustment enables the model to make more balanced and accurate predictions, even when encountering previously unseen data in the testing phase, thereby enhancing the model's robustness and generalizability.
</p>
<img class="proImg" src="projectImages/nbSmooth.webp" alt="Bernoulli NaÃ¯ve Bayes"> <br>
<b>Applications of NaÃ¯ve Bayes:  </b> <br>

<b>1. Spam Filtering </b> <br>
One of the most ubiquitous applications of NaÃ¯ve Bayes is in the realm of email spam filtering. By analyzing the frequency and presence of certain keywords within emails, NaÃ¯ve Bayes classifiers can predict with notable accuracy whether an email is spam or not. This application leverages the classifier's ability to handle large datasets and make quick predictions, making it a staple in email services' spam detection mechanisms. <br>

<b>2. Sentiment Analysis </b><br>
In the field of natural language processing (NLP), NaÃ¯ve Bayes classifiers are employed for sentiment analysis, particularly in analyzing customer reviews, social media posts, and feedback to determine the sentiment polarity (positive, negative, or neutral) of the text. By training on large corpora of labeled sentiment data, these classifiers can effectively categorize new inputs based on the learned probabilities. <br>

<b>3. Document Classification </b> <br>
NaÃ¯ve Bayes classifiers are also adept at document classification tasks, including categorizing news articles, research papers, and websites into predefined topics or genres. This capability is particularly beneficial for organizing large datasets of unstructured text and for information retrieval systems to classify documents based on their content.<br>

<b>4. Disease Prediction and Medical Diagnosis </b> <br>
In the healthcare sector, NaÃ¯ve Bayes models are applied for disease prediction and diagnostic purposes. They are used to calculate the probability of a disease given a set of symptoms or test results, aiding in the diagnostic process by providing a probabilistic assessment based on historical data. <br>

<b>5. Recommendation Systems </b>
NaÃ¯ve Bayes classifiers contribute to the recommendation systems, particularly in filtering and suggesting content to users based on their preferences and historical behavior. By analyzing user data and applying probabilistic modeling, these systems can predict and recommend products, movies, or articles that a user is likely to appreciate. <br>

<b>6. Financial Market Analysis </b> <br>
In finance, NaÃ¯ve Bayes algorithms are utilized to predict market trends and to classify news articles or reports as potentially having a positive, negative, or neutral impact on the market. This helps in automated trading decisions and sentiment analysis of the financial market. <br>

<b>7. Facial Recognition Systems </b> <br>
Although more complex models are generally preferred for image recognition, NaÃ¯ve Bayes classifiers have been employed in preliminary stages of facial recognition systems, especially in distinguishing between different facial features. They are used for classifying specific features as part of a larger ensemble of algorithms. <br>

<b>8. Fault Diagnosis </b> <br>
In engineering and manufacturing, NaÃ¯ve Bayes classifiers assist in fault diagnosis processes, identifying potential issues in machinery or systems based on the probabilistic association of various fault symptoms and known outcomes. <br>
<br>
Despite their simplicity and the underlying assumption of feature independence, NaÃ¯ve Bayes classifiers remain a powerful tool due to their efficiency, ease of implementation, and the intuitive probabilistic foundation. Understanding the applications, strengths, and limitations of NaÃ¯ve Bayes is crucial for effectively leveraging this algorithm in practical scenarios. <br>
</section>

                <header>
                  <h4 class="h4 article-title">Data</h4>
                </header>
        
                <section class="about-text">
                  <p>
                    <b>Data Preparation in Machine Learning:</b> <br>

                    Data preparation is a cornerstone process in machine learning, transforming raw datasets into a refined format amenable to algorithmic analysis. For Naive Bayes, a probabilistic model underpinned by the Bayes' Theorem, this preparatory phase assumes heightened importance due to the algorithm's distinct operational nuances. Unlike clustering, Naive Bayes thrives on supervised learning, necessitating a dataset punctuated with both features and labels to facilitate the learning of conditional probabilities. <br>
                    <br>

                    <img class="proImg" src="projectImages/initData.png" alt="intial data"> 
                    <p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>

                    The crux of data preparation for Naive Bayes involves ensuring that the dataset embodies a structure conducive to probabilistic analysis. This entails several key procedures: <br>


                  </p>
        
                  <p>
                    
                    Throughout the data preparation phase, several crucial steps were undertaken to ensure the dataset was primed for the analytical objectives set forth. Initial efforts were directed towards data cleaning, where inconsistencies and inaccuracies were rectified, alongside the removal of duplicate entries. A significant portion of this phase was dedicated to standardization, a process designed to mitigate potential biases and disparities in scale among the dataset's features. By scaling each feature to have a mean of zero and a standard deviation of one, a level playing field was established, ensuring equitable contribution of variables to the analytical process. <br>
                    <br>
                    Further, the dataset underwent encoding procedures to facilitate the incorporation of categorical data into the modeling process. Label encoding was applied to ordinal variables, including 'Education_Level' and 'Income_Category', translating these into a numerical format that preserves order. For nominal variables such as 'Gender', 'Marital_Status', and 'Card_Category', one-hot encoding was utilized, expanding these categories into separate binary features, thus avoiding any imposition of artificial ordinality.<br>
                    <br>
                    The challenge of an imbalanced class distribution within the "Attrition_Flag" column necessitated additional interventions. The disproportionate representation of "Existing Customer" instances over "Attrited Customer" instances presented a risk of model bias, potentially undermining the model's predictive accuracy for the minority class. To counteract this, Synthetic Minority Over-sampling Technique (SMOTE) was employed, generating synthetic instances of the underrepresented class, thus achieving a balanced class distribution with 8500 samples per class, elevating the dataset's total to 17000 samples.<br>
                    <br>
                    Subsequent to addressing the imbalance, the dataset was partitioned into training and testing sets, adhering to an 80:20 ratio. This division was instrumental in allocating a substantial portion of data for model training while reserving a significant fraction for the impartial evaluation of model performance. <br>
                    <br>
                    An integral component of the feature selection process involved the identification and elimination of redundant features, with 'CLIENTNUM' being a prime example. Its removal was predicated on its lack of relevance to the patterns of interest, thereby enhancing the model's generalizability and reducing the likelihood of overfitting. <br>
                    <br>
                    The feature selection phase employed a tripartite methodology, incorporating Random Forest for determining feature importance, Recursive Feature Elimination (RFE) for its systematic reduction of the feature space, and correlation analysis with the target variable for its efficiency in identifying linear relationships. This comprehensive approach culminated in the distillation of two distinct feature sets: Set 1, comprising features of mutual significance across all three methods, and Set 2, encompassing features unanimously recognized as pivotal in the context of customer churn prediction. This dual-faceted strategy ensured the retention of features that are intrinsically relevant to the target variable, spanning both linear and nonlinear dimensions. <br>
                   <img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
                   <b> Features Set-1 :</b> <br>
                   <img class="proImg" src="projectImages/set1.png" alt="Final data">
                   <b> Features Set-2 :</b> <br>
                   <img class="proImg" src="projectImages/set2.png" alt="Final data">
                   <p style="text-align: center;">The above are the sample of the data, having undergone preprocessing steps as described earlier, are now prepared and will be used for Naive bayes classifier.</p>

                  </p>

                  <p>

                    <b>Creating and Understanding the Importance of Disjoint Test-Train Splits </b> <br>

                    <br>
In the construction of predictive models, the partitioning of data into training and testing subsets is an indispensable step. For this analysis, an 80:20 split ratio was employed, allocating 80% of the data for training and the remaining 20% for testing. This division was methodically executed to ensure a substantial volume of data for the model to learn from, while still preserving a considerable portion for validation purposes. The training set functions as the foundational dataset upon which the model is built and refined, allowing the learning algorithms to discern and assimilate the intricate relationships between the features and the target variable. <br>
<br>
The importance of creating a disjoint split between the training and testing sets lies in its capacity to provide an objective evaluation of the model's performance. A disjoint split guarantees that the testing set is a collection of observations that the model has not previously encountered during the training phase. This prevents the model from simply memorizing specific data pointsâ€”a phenomenon known as overfittingâ€”and instead encourages the development of generalization capabilities. By evaluating the model on the testing set, we gain insight into how well it can apply its learned patterns to new, unseen data, which is a robust indicator of its potential efficacy in real-world applications. <br>
<br>
                  </p>


                  <b>Training Data</b> <br>
                  <img class="proImg" src="projectImages/trainData.png" alt="Final data">

                  <b>Testing data</b> <br>
                  <img class="proImg" src="projectImages/testData.png" alt="Final data">


                </section>

                <header>
                  <h4 class="h4 article-title">Code</h4>
                </header>
        
                <section class="about-text">
                  <p>
                    The implementation of Naive Bayes, along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying Naive Bayes technique within a dataset.
        

                    <h6 class="h6"><a href="https://colab.research.google.com/drive/1EgBUMcy-Msn0hl15PiwfZTIYbPlUhdTi?usp=sharing" target="_blank">Click here to see the code for Naive Bayes </a></h6>
                   

                </section>

                <header>
                  <h4 class="h4 article-title">Results</h4>
                </header>
        
                <section class="about-text">
                  <p>
                  


<b>Set 1: </b> <br>

<b>The confusion matrix for Set 1 shows: </b> <br>
<br>
- True Negatives (Top-Left Square): There are 1119 instances where the model correctly predicted the negative class. This reflects the model's capability to identify scenarios where the event of interest is not expected to occur.<br>
<br>
- True Positives (Bottom-Right Square): The model accurately predicted the positive class in 1043 cases, signifying its effectiveness in recognizing patterns that suggest the occurrence of the event. <br>
<br>
- False Positives (Top-Right Square): In 605 instances, the model erroneously predicted the positive class. This number indicates that while the model is somewhat prone to type I errors, it is not excessively so. <br>
<br>
- False Negatives (Bottom-Left Square): There were 633 occurrences where the model missed predicting the positive class. This number contributes to a lower recall, signaling that the model overlooks a considerable number of actual positive instances. <br>
<br>
<b>From these insights: </b> <br>

- The model shows a balanced tendency between precision and recall, with a slight lean towards precision. This suggests that when it predicts the occurrence of an event, it is relatively reliable. However, it also misses a fair amount of actual occurrences, which could be critical depending on the application context. <br>
<br>
- The accuracy of approximately 63.6% demonstrates that the model's predictions are correct more often than not, but there is room for improvement, especially in contexts where higher accuracy is crucial. <br>
<br>

<b>Set 2: </b> <br>

<b>The confusion matrix for Set 2 reveals: </b> <br>

- True Negatives (Top-Left Square): The model correctly predicted the negative class in 704 cases. This number indicates a decent ability to detect when the event of interest is not present. <br>
<br>
- True Positives (Bottom-Right Square): There are 1194 correct positive predictions, showing that the model can identify a large number of true positive instances. <br>
<br>
- False Positives (Top-Right Square): With 1020 false-positive predictions, the model appears to over-predict the positive class, which could lead to resource misallocation or other types of inefficiencies in practical applications. <br>
<br>
- False Negatives (Bottom-Left Square): The model failed to predict 482 actual positive instances. While significant, this is lower than the false positives, which aligns with a higher recall and indicates the model's sensitivity towards detecting positive instances. <br>
<br>

<b>From these points: </b> <br>

- The model exhibits a tendency to predict the positive class, as evidenced by a higher recall (71.2%) but at the cost of precision (53.9%). This could suggest a need to balance out the model to reduce false positives. <br>
<br>
- With an accuracy of about 55.8%, the model performs somewhat better than a coin flip. This level of accuracy indicates the necessity for further refinement to ensure the model is practical and reliable for decision-making purposes. <br>
<br>

The ROC AUC scores for both sets show moderate predictive capability, with Set 1 performing slightly better than Set 2. These results, taken together with the insights from the confusion matrices, suggest specific areas where model adjustments and further investigations could yield improvements in performance. <br>
<br>

</p>

<img class="resImg"  src="projectImages/nbResults.png" alt="Naive Bayes">
<p style="text-align: center;">The visual representation of the Naive Bayes models' performance, as encapsulated by the confusion matrices and ROC curves.</p> 
     

</section>

<header>
  <h4 class="h4 article-title">Conclusion</h4>
</header>

<section class="about-text">
  <p>
    The application of Naive Bayes models on the two distinct feature sets has yielded informative insights into their predictive capabilities. The confusion matrices unveiled the balance between the true positives and negatives against the respective errors, illuminating the strengths and weaknesses in classification. Notably, Set 1 demonstrated a commendable balance between precision and recall, suggesting a model that, while not overconfident, may benefit from improved sensitivity. Conversely, Set 2 showed a higher recall indicative of the model's ability to identify the majority of positive instances, but with a substantial number of false positives, signaling a potential overestimation.<br>
    <br>
    
    The ROC curves and corresponding AUC scores further illustrated the models' discriminative power. Set 1, with a slightly higher AUC, indicates a better overall performance in distinguishing between classes compared to Set 2. However, neither model achieved an AUC suggesting excellent predictive performance, which implies room for further enhancement. <br>
    <br>
    
    The accuracy metrics reinforce the narrative that while the models have learned from the data, the modest scores point to a necessity for refinement. Potential avenues for improvement could involve additional feature engineering, hyperparameter tuning, or exploring alternative modeling approaches. <br>
    <br>
    
    Ultimately, these insights provide a valuable foundation for iterative model development, guiding towards a more nuanced understanding of the underlying data patterns and the models' behavior. This iterative process is essential for advancing towards a Naive Bayes classifier that performs with higher accuracy and reliability. <br>
    <br>

  </p>
        
  </section>       

          </li>
            
            <li class="project-item  active" data-filter-item data-category="decision trees">
                <header>
                    <h3 class="h3 article-title">Decision Trees</h3>
                  </header>
          
                  <header>
                    <h4 class="h4 article-title">Overview</h4>
                  </header>
                  <section class="about-text">
          
                  <p>
                    <b>What is Decision Tree?</b>
                  </p>

                  <p>


A Decision Tree is an algorithm within supervised machine learning, predominantly utilized for classification and regression tasks. It constructs a model that predicts the value of a target variable based on several input variables. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a target value. The model resembles a tree structure with branches representing decision paths and leaves representing outcomes.
<br>
<br>
The algorithm initiates from a root node and splits the dataset into subsets using a feature-based decision-making process. This splitting, illustrated in Figure 1, continues recursively on each derived subset in a manner that maximizes the homogeneity of the target variables within the subsets. At every node, the decision tree algorithm selects the optimal feature that results in the most effective division of data, considering criteria such as information gain or Gini impurity.

<br>
<br>

This iterative process of partitioning data into finer subsets proceeds until the algorithm reaches a termination criterion, which could be a set depth of the tree or when no further significant information gain is possible. At this juncture, the algorithm makes a prediction based on the majority target value within a leaf node.
<br>
<br>
Through its hierarchical structure, the decision tree algorithm provides a clear and interpretable framework for decision-making, revealing the sequence of rule-based steps that lead to a prediction. This attribute, along with its flexibility and ease of use, makes the decision tree a fundamental tool in the machine learning domain for deriving insights from complex datasets.
<br>

                  </p>
        
                  <img class="proImg" src="projectImages/dtIntro.webp" alt="Naive Bayes">
                  <p style="text-align: center;">Fig.1. Simple Representation of the Decision Tree</p>
                  <p>




<b>Measures for Splitting the data:</b> <br>

<b>GINI Index:</b> <br>

The GINI Index is a statistical measure used to quantify the impurity or diversity of a dataset with respect to the classes it contains. It is defined by the formula: <br>

<img class="eqnImg" src="projectImages/dt1.webp" alt="Gaussian Naive Bayes">

Here, p_i represents the probability of an object being classified into a specific class within the node. The GINI Index values range from 0 to 1, where a value of 0 indicates perfect homogeneity (all elements belong to a single class), and a value of 1 denotes maximal diversity (equal distribution among classes).
<br>

<b>Entropy:</b> <br>

Entropy measures the level of uncertainty or disorder in a dataset and is calculated as: <br>



<img class="eqnImg" src="projectImages/dt2.webp" alt="Multinomial NaÃ¯ve Bayes">
<br>

In this context, p_i is the probability of occurrence of event i. A dataset with uniform class distribution has high entropy, while entropy is zero when all observations belong to a single class, indicating complete order.

<br>
<br>

<b>Information Gain:</b> <br>

Information Gain evaluates the decrease in entropy after a dataset is divided on an attribute. Essentially, it measures the effectiveness of an attribute in classifying the data. The formula for Information Gain is: <br>


<img class="eqnImg" src="projectImages/dt3.webp" alt="Bernoulli NaÃ¯ve Bayes"> <br>
<br>
A higher Information Gain indicates a more significant reduction in entropy, suggesting that the dataset has been split into subsets that are more homogeneous than the original. <br>

<br>

In the construction of decision trees, these metrics play a pivotal role in determining the most appropriate attribute for data splitting at each node. Attributes are evaluated based on their ability to decrease impurity (GINI Index) or disorder (Entropy), with the aim of maximizing Information Gain. The attribute that offers the highest Information Gain or lowest GINI Index is selected for the split. This recursive process of splitting continues until the tree reaches its full extent or meets a stopping criterion, ultimately forming a tree structure that efficiently categorizes the data into homogenous subsets. <br>
<br>

<b>Applications of Decision Trees:  </b> <br>


<b>1. Customer Segmentation:  </b> <br>

Decision trees excel in marketing analytics, particularly in segmenting customers based on various characteristics such as purchasing behavior, preferences, and demographic data. This segmentation enables businesses to craft targeted marketing strategies, enhancing customer engagement and optimizing resource allocation. <br>

<b>2. Fraud Detection: </b> <br>

In the financial sector, decision trees play a crucial role in identifying fraudulent transactions by analyzing patterns and anomalies in spending behavior, account activity, and transaction details. This capability is pivotal in preventing financial losses and maintaining the integrity of financial systems. <br>

<b>3. Loan Approval Processes:  </b> <br>

Decision trees are instrumental in the banking and finance industries for evaluating the creditworthiness of loan applicants. Factors like credit score, employment history, income levels, and existing debt are analyzed to make informed decisions on loan approvals, reducing the risk of default. <br>

<b>4. Diagnosis of Medical Conditions:  </b> <br>

Healthcare professionals use decision trees to diagnose diseases and medical conditions by examining symptoms, lab results, and patient histories. This application facilitates quicker, more accurate medical diagnoses and personalized treatment plans, significantly impacting patient care. <br>

<b>5. Predictive Maintenance:  </b> <br>
In manufacturing and industrial settings, decision trees are used for predictive maintenance, identifying potential equipment failures before they occur by analyzing operational data, maintenance records, and sensor readings. This predictive approach helps in minimizing downtime, extending equipment life, and reducing maintenance costs. <br>
 <br>
These applications demonstrate the power of decision trees in providing insightful, data-driven decisions across a wide range of industries, making them invaluable tools in the machine learning landscape.
<br>

</section>

<header>
  <h4 class="h4 article-title">Data</h4>
</header>

<section class="about-text">
  <p>
    <b>Data Preparation in Machine Learning:</b> <br>

    Data preparation is a critical stage in the machine learning pipeline, especially for models like Decision Trees, which are utilized for both classification and regression tasks. This process involves transforming raw data into a format that is suitable for the model to process, learn from, and make predictions. Decision Trees, being non-parametric and capable of handling both numerical and categorical data, still benefit greatly from thoughtful data preparation to improve their performance and accuracy.

 <br>
    <br>

    <img class="proImg" src="projectImages/initData.png" alt="intial data"> 
    <p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>

    The data preparation for Decision Trees encompasses several key activities: <br>

  </p>

  <p>
    
    Throughout the data preparation phase, several crucial steps were undertaken to ensure the dataset was primed for the analytical objectives set forth. Initial efforts were directed towards data cleaning, where inconsistencies and inaccuracies were rectified, alongside the removal of duplicate entries. A significant portion of this phase was dedicated to standardization, a process designed to mitigate potential biases and disparities in scale among the dataset's features. By scaling each feature to have a mean of zero and a standard deviation of one, a level playing field was established, ensuring equitable contribution of variables to the analytical process. <br>
    <br>
    Further, the dataset underwent encoding procedures to facilitate the incorporation of categorical data into the modeling process. Label encoding was applied to ordinal variables, including 'Education_Level' and 'Income_Category', translating these into a numerical format that preserves order. For nominal variables such as 'Gender', 'Marital_Status', and 'Card_Category', one-hot encoding was utilized, expanding these categories into separate binary features, thus avoiding any imposition of artificial ordinality.<br>
    <br>
    The challenge of an imbalanced class distribution within the "Attrition_Flag" column necessitated additional interventions. The disproportionate representation of "Existing Customer" instances over "Attrited Customer" instances presented a risk of model bias, potentially undermining the model's predictive accuracy for the minority class. To counteract this, Synthetic Minority Over-sampling Technique (SMOTE) was employed, generating synthetic instances of the underrepresented class, thus achieving a balanced class distribution with 8500 samples per class, elevating the dataset's total to 17000 samples.<br>
    <br>
    Subsequent to addressing the imbalance, the dataset was partitioned into training and testing sets, adhering to an 80:20 ratio. This division was instrumental in allocating a substantial portion of data for model training while reserving a significant fraction for the impartial evaluation of model performance. <br>
    <br>
    An integral component of the feature selection process involved the identification and elimination of redundant features, with 'CLIENTNUM' being a prime example. Its removal was predicated on its lack of relevance to the patterns of interest, thereby enhancing the model's generalizability and reducing the likelihood of overfitting. <br>
    <br>
    The feature selection phase employed a tripartite methodology, incorporating Random Forest for determining feature importance, Recursive Feature Elimination (RFE) for its systematic reduction of the feature space, and correlation analysis with the target variable for its efficiency in identifying linear relationships. This comprehensive approach culminated in the distillation of two distinct feature sets: Set 1, comprising features of mutual significance across all three methods, and Set 2, encompassing features unanimously recognized as pivotal in the context of customer churn prediction. This dual-faceted strategy ensured the retention of features that are intrinsically relevant to the target variable, spanning both linear and nonlinear dimensions. <br>
   <img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
   <b> Features Set-1 :</b> <br>
   <img class="proImg" src="projectImages/set1.png" alt="Final data">
   <b> Features Set-2 :</b> <br>
   <img class="proImg" src="projectImages/set2.png" alt="Final data">
   <p style="text-align: center;">The above are the sample of the data, having undergone preprocessing steps as described earlier, are now prepared and will be used for Decision Tree.</p>

  </p>
  <p>

    <b>Creating and Understanding the Importance of Disjoint Test-Train Splits </b> <br>

    <br>
In the construction of predictive models, the partitioning of data into training and testing subsets is an indispensable step. For this analysis, an 80:20 split ratio was employed, allocating 80% of the data for training and the remaining 20% for testing. This division was methodically executed to ensure a substantial volume of data for the model to learn from, while still preserving a considerable portion for validation purposes. The training set functions as the foundational dataset upon which the model is built and refined, allowing the learning algorithms to discern and assimilate the intricate relationships between the features and the target variable. <br>
<br>
The importance of creating a disjoint split between the training and testing sets lies in its capacity to provide an objective evaluation of the model's performance. A disjoint split guarantees that the testing set is a collection of observations that the model has not previously encountered during the training phase. This prevents the model from simply memorizing specific data pointsâ€”a phenomenon known as overfittingâ€”and instead encourages the development of generalization capabilities. By evaluating the model on the testing set, we gain insight into how well it can apply its learned patterns to new, unseen data, which is a robust indicator of its potential efficacy in real-world applications. <br>
<br>
  </p>

  <b>Training Data</b> <br>
  <img class="proImg" src="projectImages/trainData.png" alt="Final data">

  <b>Testing data</b> <br>
  <img class="proImg" src="projectImages/testData.png" alt="Final data">
</section>

<header>
  <h4 class="h4 article-title">Code</h4>
</header>

<section class="about-text">
  <p>
    The implementation of Decision Trees, along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying Decision Trees.
  </p>


    <h6 class="h6"><a href="https://colab.research.google.com/drive/10tPqF_6rZ7cuAe15huYNTI5lTRdwqBA1?usp=sharing" target="_blank">Click here to see the code for Decision Tree </a></h6>
   

</section>

                  <header>
                    <h4 class="h4 article-title">Results</h4>
                  </header>
          
                 
                  <section class="about-text">
                    <p>
                      The results from the application of Decision Tree models on the two distinct feature sets elucidate the nuanced predictive capabilities of these classifiers. Training each Decision Tree on a different subset of data has afforded a comprehensive learning experience, reducing the propensity for overfitting and encompassing a breadth of variability within the dataset. <br>
                      <br>
                      
                      
                      For the Decision Tree associated with the first feature set, the metrics provide a compelling narrative of its performance: <br>
                      <br>
                      - Accuracy (75.8%): The model's predictions are reliable three-quarters of the time, indicating a robust general performance. <br>
                      - Precision (75.1%): This high precision level demonstrates that when the model predicts a positive outcome, it is correct most of the time, implying a reduced frequency of false-positive errors. <br>
                      - Recall (76.3%): The model has a strong ability to identify the majority of actual positive cases, suggesting that it is effectively capturing the occurrence of the events of interest. <br>
                      - F1 Score (75.7%): The F1 Score, being a harmonic mean of precision and recall, affirms the model's balanced performance in terms of both accuracy and completeness of the positive class detection. <br>
                      - ROC AUC Score (75.8%): The area under the ROC curve suggests that the model has a commendable discriminative ability, distinguishing between the classes with a high degree of accuracy. <br>
                      <br>
                      
                      In comparison, the Decision Tree developed from the second feature set exhibits enhanced performance across all metrics: <br>
                      <br>
                      - Accuracy (94.5%): This near-perfect accuracy denotes that the model is highly reliable in its predictions.<br>
                      - Precision (93.9%): Such a high precision indicates that the model's predictive affirmations are extremely trustworthy. <br>
                      - Recall (95.0%): An excellent recall rate reflects the model's proficiency in identifying nearly all positive instances. <br>
                      - F1 Score (94.5%): A high F1 Score showcases the model's exceptional balance in precision and recall, marking it as both accurate and comprehensive. <br>
                      - ROC AUC Score (94.5%): The AUC value underscores the model's superior capability to discriminate between the event occurrences with a high degree of certainty. <br>
                      <br>
                      The training approach, coupled with rigorous testing, ensured that the performance metrics derived are reflective of the models' potential in real-world application scenarios. The disjoint nature of the training and testing sets serves as a bulwark against information leakage, ensuring that the models' evaluations are grounded in their ability to generalize to unseen data.<br>
                      <br>
                      Visualizing the Decision Trees provides transparent insight into the decision-making process, illuminating the pivotal features and their thresholds that guide predictions. This clarity allows for an appreciation of the models' inner workings and an evaluation of the feature space's influence on the outcome. <br>
                      <br>
                      
                      Collectively, the performance metrics and visualizations underscore the Decision Trees' effectiveness in predicting outcomes, with the second model displaying particularly strong predictive prowess. These results signal the models' viability for practical deployment, with the second set demonstrating potential for high-stakes decision-making where accuracy is paramount. <br>
                      <br>
                    </p>
                    <img class="resImg"  src="projectImages/dtResults.png" alt="Naive Bayes">
                    <img class="proImg" src="projectImages/dtOutput.png" alt="Final data">
                    <img class="proImg" src="projectImages/dtOutput2.png" alt="Final data">
                    <img class="proImg" src="projectImages/dtoutput3.png" alt="Final data">
                    <p style="text-align: center;">The visual representation of the Decision Tree models' performance, as encapsulated by the confusion matrices and ROC curves.</p> 
                    
                  </section>


                  <header>
                    <h4 class="h4 article-title">Conclusion</h4>
                  </header>
                  
                  <section class="about-text">
                    <p>
                      
                      The first Decision Tree, while demonstrating a reasonable degree of accuracy, precision, and recall, suggests there is an opportunity to refine the model further. This could involve revisiting the feature selection process, tuning the model's hyperparameters, or addressing any data imbalances and noise that may be impacting its performance.<br>

                      <br>

                      The second Decision Tree's impressive metrics indicate a model that is both sensitive to the presence of the positive class and conservative in its predictions of the negative class. Such a model is incredibly valuable in scenarios where false negatives and false positives carry significant consequences, affirming its readiness for deployment in scenarios where high precision is crucial. <br>

                      <br>

                      The analysis of confusion matrices for both models has revealed their respective strengths in classifying true positives and negatives, alongside a clear depiction of their error types through false positives and negatives. The visualization of the Decision Trees themselves has offered transparency into the decision-making logic of the models, allowing for a better understanding of how the features influence predictions. <br>

                      <br>



                      Moving forward, the insights gleaned from this exercise suggest that Decision Trees can serve as a reliable method for classification tasks, provided that they are appropriately trained and validated. For future applications, the second feature set model, in particular, stands out as a promising candidate for deployment, with its predictive performance suggesting it could serve as a dependable tool in making informed decisions.<br>

                      <br>
                  
                    </p>
                          
                    </section>  
            </li>



            <li class="project-item  active" data-filter-item data-category="support vector machines (svm)">
                <header>
                    <h2 class="h2 article-title">Support Vector Machines (SVM)</h2>
                  </header>
          
                  <header>
                    <h4 class="h4 article-title">Overview</h4>
                  </header>
                  <section class="about-text">
          
                  <p>
                    <b>What are SUPPORT VECTOR MACHINES (SVM)?</b>
                  </p>

                  <p>


                    Support Vector Machines (SVMs) are a cornerstone of supervised learning, renowned for their robust performance in both classification and regression tasks. The principal objective of SVM is to identify an optimal hyperplane that maximizes the separation between different classes within the feature space.
<br>
<br>

This optimal hyperplane is determined by maximizing the margin, defined as the distance between the hyperplane and the nearest data points from each class, known as support vectors. These support vectors are pivotal as they delineate the boundary of the class separation.

<br>
<br>

SVMs excel by constructing a hyperplane with the widest possible margin between the nearest data points of different classes, ensuring robust class separation. Effective in handling both linear and non-linear distributions, SVMs leverage kernel functions to operate in multi-dimensional space, thus enhancing their applicability to a diverse range of complex datasets.
<br>
<br>

                  </p>
        
                  <img class="proImg" src="projectImages/svmIntro.webp" alt="SVM">
                  <p style="text-align: center;">Fig.1. Simple Representation of the SVM Classification</p>
                  <p>




<b> How SVMs Work:</b> <br>
<br>

<b>Linear Separators:</b> <br>

At the foundation of Support Vector Machine (SVM) methodology is the quest to establish a hyperplane that adeptly segregates the classes in a feature space. In the context of a binary classification challenge, the SVMâ€™s objective is to identify a hyperplane that maximizes the marginâ€”the most substantial distance possible between the nearest members of both classes. This hyperplane serves as a critical decision boundary: data points located on one side are classified into one category, while those on the opposite side fall into another. The data points nearest to the hyperplane are termed "support vectors," as they crucially influence the placement and orientation of the hyperplane. They are pivotal because they "support" the model in the decision-making process, hence the nomenclature. <br>
<br>

<b>Linear vs. Non-linear Classification:</b> <br>

Primarily, SVMs are linear classifiers, renowned for their capability to craft a division with a maximal margin. However, they are versatile enough to tackle non-linear classification scenarios through the implementation of the kernel trick. This technique allows SVMs to operate effectively in environments where the data points are not linearly separable. By applying a kernel function, SVMs can transform the original feature space into a higher dimension where a linear hyperplane can be feasibly constructed, thereby enabling accurate classification even in complex, non-linear datasets. This adaptability makes SVMs immensely powerful and widely applicable across various analytical challenges in machine learning.<br>



<img class="proImg" src="projectImages/svm2.webp" alt="SVM-2">

<br>

<b>The Kernel Trick in Support Vector Machines:</b> <br>

The kernel trick is an ingenious technique employed by Support Vector Machines (SVMs) to facilitate the learning of non-linear decision boundaries using the foundational algorithms intended for linear classifiers. This method involves an implicit transformation of the input features into higher-dimensional feature spaces where linear separability might be more feasible. <br>
<br>

<b>How the Kernel Works:</b> <br>

Kernel functions play a pivotal role in this process. They accept input vectors from the original feature space and compute the dot product as if the vectors were in a transformed, higher-dimensional space. Essentially, these kernels project the original data vectors into a new space without the need to explicitly compute their coordinates in that space, which can significantly simplify the computation. <br>

<img class="proImg" src="projectImages/svmKernal.webp" alt="SVMs Working">
<p style="text-align: center;">Illustration of Kernel Method in Support Vector Machine</p>



<b>Commonly Used Kernels: </b> <br>


<b>1. Linear Kernel:  </b> <br>

This kernel performs no transformation; it simply calculates the dot product of the vectors as they are. It is ideal for datasets where the classes are already linearly separable. <br>

<b>2. Polynomial Kernel: </b> <br>

This kernel transforms the input vectors into a polynomial feature space of a specified degree (e.g., quadratic, cubic), allowing the model to capture interactions between features up to that degree. It's useful for datasets where relationships between features can be captured by polynomial equations. <br>

<b>3. Radial Basis Function (RBF) or Gaussian Kernel:  </b> <br>

One of the most versatile kernels, the RBF kernel measures the exponential decay of the distance between vectors in the feature space. It is exceptionally well-suited for handling complex relationships where the class labels and attributes do not have a straightforward linear correlation. <br>

<b>4. Sigmoid Kernel:  </b> <br>

Mirroring the activation function seen in neural networks, the sigmoid kernel projects data into higher dimensions in a manner similar to the logistic function. It can be useful in certain contexts but also needs careful parameter tuning to avoid issues related to neural network training, like vanishing gradients. <br>

<br>
Each kernel has unique characteristics and is chosen based on the specific requirements of the dataset and the complexity of the classification problem at hand. By selecting an appropriate kernel, SVMs can be incredibly effective across a wide range of nonlinear classification challenges.
<br>
<img class="proImg" src="projectImages/svmKernalTypes.webp" alt="Types of SVMs kernals "> 
<p style="text-align: center;">Commonly used kernels</p>


<b>Advantages of Using Kernels in SVMs: </b> <br>

The kernel trick offers significant advantages in the implementation of Support Vector Machines (SVMs). It allows SVMs to form complex decision boundaries while utilizing linear methods at their core. This capability facilitates the handling of non-linear classification problems with the same simplicity and efficiency as linear problems. By leveraging kernel functions, SVMs can model intricate relationships within the data without requiring an explicit computation of high-dimensional space coordinates. This indirect computation method significantly reduces the computational burden, making the process both efficient and feasible even when dealing with large datasets. <br>
<br>

<b>Choosing the Right Kernel:</b> <br>

The selection of a kernel and its parameters is pivotal in determining the performance of an SVM. There is no universal kernel that fits all types of data; the choice largely depends on the specific characteristics of the dataset and the problem at hand. Factors influencing the choice of a kernel include the nature of the data (linear or non-linear, the degree of class overlap), the complexity of the decision boundaries required, and empirical tuning based on model performance in training and validation phases. Experimentation through cross-validation and grid search techniques often helps in identifying the optimal kernel and tuning its parameters.<br>

<b>Flexibility and Broad Applicability:</b> <br>

The flexibility to choose from various kernels makes SVMs extremely versatile and powerful, suitable for a wide range of classification and regression tasks. This adaptability is why SVMs are prevalent in many fields, from image recognition, where they process complex pixel data, to bioinformatics, where they analyze intricate genetic patterns. By allowing a tailored approach to different types of data and problems, SVMs with kernel tricks stand out as a robust tool in the arsenal of machine learning methodologies. <br>
<br>

<b>Importance of the Dot Product in SVMs </b> <br>


<b>Geometric Interpretation:  </b> <br>

In the realm of Support Vector Machines (SVMs), the dot product serves a crucial role by measuring the angle between vectors within the feature space. This measurement is fundamental in constructing the decision boundary. Specifically, the dot product aids in determining the cosine of the angle between vectors, which is essential for assessing how data points are oriented and positioned relative to one another. This geometric relationship directly influences the SVM's decision on how to separate the data points, making the dot product integral to understanding and defining the spatial relationships within the dataset. <br>

<b>Computational Efficiency: </b> <br>

From a computational perspective, the dot product is highly efficient and enables SVMs to swiftly evaluate the relationship between vectors. This efficiency is particularly vital when dealing with large datasets, where the speed of computation can significantly impact overall performance. The simplicity of the dot product calculation allows for rapid assessments of vector relationships, thereby enhancing the speed with which the SVM can process and analyze data. <br>

<b>Kernel Trick:  </b> <br>

The dot product also plays a pivotal role in the kernel trick, a technique that SVMs use to handle non-linear data separations. By employing the kernel function, which utilizes dot products, the SVM can project data into a higher-dimensional space without the need for explicit computation of the coordinates in that space. This aspect is crucial because computations in high-dimensional spaces can be computationally demanding and practically challenging. The kernel trick, facilitated by the dot product, allows the SVM to operate efficiently in these expanded spaces, enabling it to manage complex, non-linear decision boundaries effectively. This makes the dot product not only a tool for geometric interpretation and computational efficiency but also a foundational component that supports the advanced capabilities of SVMs in handling diverse and complex data structures. <br>

<br>

<b>Kernel Functions in SVMs </b> <br>

Kernel functions, represented as  K(x, y), are fundamental in SVMs for computing the dot product of vectors x and y in a high-dimensional feature space. This process corresponds to a non-linear transformation of the input vectors. By doing so, the kernel function allows SVMs to handle non-linearly separable data effectively, mapping input vectors into a new space where a linear separator is sufficient. This capability is crucial for dealing with complex datasets where traditional linear models fail, providing SVMs with the flexibility to operate across various data structures without direct computation in the expanded feature space. <br>

<br>
<b>Polynomial Kernel:</b> <br>

<img class="fImg" src="projectImages/polykernal.png" alt="SVMs Working">


<b>Radial Basis Function (RBF) or Gaussian Kernel</b> <br>

<img class="fImg" src="projectImages/rbfKernal.png" alt="SVMs Working"> <br>

<b>Example of taking a 2D points and a polynomial kernel with r = 1 and d = 2</b>

<img class="exImg" src="projectImages/svmEx.png" alt="SVMs Example"> 





</section>

<header>
  <h4 class="h4 article-title">Data</h4>
</header>

<section class="about-text">
  <p>
    <b>Data Preparation in Machine Learning:</b> <br>

Data preparation is a critical stage in the machine learning pipeline, especially for models like Support Vector Machines (SVMs), which are utilized for both classification and regression tasks. This process involves transforming raw data into a format that is suitable for the model to process, learn from, and make predictions. SVMs, known for their effectiveness in high-dimensional spaces and their requirement for careful feature scaling, benefit greatly from meticulous data preparation to improve their performance and accuracy.

 <br>
    <br>

    <img class="proImg" src="projectImages/initData.png" alt="intial data"> 
    <p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>

    The data preparation for SVM encompasses several key activities: <br>

  </p>

  <p>
    
    Throughout the data preparation phase, several crucial steps were undertaken to ensure the dataset was primed for the analytical objectives set forth. Initial efforts were directed towards data cleaning, where inconsistencies and inaccuracies were rectified, alongside the removal of duplicate entries. A significant portion of this phase was dedicated to standardization, a process designed to mitigate potential biases and disparities in scale among the dataset's features. By scaling each feature to have a mean of zero and a standard deviation of one, a level playing field was established, ensuring equitable contribution of variables to the analytical process. <br>
    <br>
    Further, the dataset underwent encoding procedures to facilitate the incorporation of categorical data into the modeling process. Label encoding was applied to ordinal variables, including 'Education_Level' and 'Income_Category', translating these into a numerical format that preserves order. For nominal variables such as 'Gender', 'Marital_Status', and 'Card_Category', one-hot encoding was utilized, expanding these categories into separate binary features, thus avoiding any imposition of artificial ordinality.<br>
    <br>
    The challenge of an imbalanced class distribution within the "Attrition_Flag" column necessitated additional interventions. The disproportionate representation of "Existing Customer" instances over "Attrited Customer" instances presented a risk of model bias, potentially undermining the model's predictive accuracy for the minority class. To counteract this, Synthetic Minority Over-sampling Technique (SMOTE) was employed, generating synthetic instances of the underrepresented class, thus achieving a balanced class distribution with 8500 samples per class, elevating the dataset's total to 17000 samples.<br>
    <br>
    Subsequent to addressing the imbalance, the dataset was partitioned into training and testing sets, adhering to an 80:20 ratio. This division was instrumental in allocating a substantial portion of data for model training while reserving a significant fraction for the impartial evaluation of model performance. <br>
    <br>
    An integral component of the feature selection process involved the identification and elimination of redundant features, with 'CLIENTNUM' being a prime example. Its removal was predicated on its lack of relevance to the patterns of interest, thereby enhancing the model's generalizability and reducing the likelihood of overfitting. <br>
    <br>
    The feature selection phase employed a tripartite methodology, incorporating Random Forest for determining feature importance, Recursive Feature Elimination (RFE) for its systematic reduction of the feature space, and correlation analysis with the target variable for its efficiency in identifying linear relationships. This comprehensive approach culminated in the distillation of two distinct feature sets: Set 1, comprising features of mutual significance across all three methods, and Set 2, encompassing features unanimously recognized as pivotal in the context of customer churn prediction. This dual-faceted strategy ensured the retention of features that are intrinsically relevant to the target variable, spanning both linear and nonlinear dimensions. <br>
   <img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
   <b> Features Set-1 :</b> <br>
   <img class="proImg" src="projectImages/set1.png" alt="Final data">
   <b> Features Set-2 :</b> <br>
   <img class="proImg" src="projectImages/set2.png" alt="Final data">
   <p style="text-align: center;">The above are the sample of the data, having undergone preprocessing steps as described earlier, are now prepared and will be used for Support Vector Machines.</p>

  </p>
  <p>

    <b>Creating and Understanding the Importance of Disjoint Test-Train Splits </b> <br>

    <br>
In the construction of predictive models, the partitioning of data into training and testing subsets is an indispensable step. For this analysis, an 80:20 split ratio was employed, allocating 80% of the data for training and the remaining 20% for testing. This division was methodically executed to ensure a substantial volume of data for the model to learn from, while still preserving a considerable portion for validation purposes. The training set functions as the foundational dataset upon which the model is built and refined, allowing the learning algorithms to discern and assimilate the intricate relationships between the features and the target variable. <br>
<br>
The importance of creating a disjoint split between the training and testing sets lies in its capacity to provide an objective evaluation of the model's performance. A disjoint split guarantees that the testing set is a collection of observations that the model has not previously encountered during the training phase. This prevents the model from simply memorizing specific data pointsâ€”a phenomenon known as overfittingâ€”and instead encourages the development of generalization capabilities. By evaluating the model on the testing set, we gain insight into how well it can apply its learned patterns to new, unseen data, which is a robust indicator of its potential efficacy in real-world applications. <br>
<br>
  </p>

  <b>Training Data</b> <br>
  <img class="proImg" src="projectImages/trainData.png" alt="Final data">

  <b>Testing data</b> <br>
  <img class="proImg" src="projectImages/testData.png" alt="Final data">
</section>

<header>
  <h4 class="h4 article-title">Code</h4>
</header>

<section class="about-text">
  <p>
    The implementation of Support Vector Machines, along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying Support Vector Machines.
  </p>


    <h6 class="h6"><a href="https://colab.research.google.com/drive/1cffwu2EfXWOpOSjdPsKxrb1P5uvRgwGs?usp=sharing" target="_blank">Click here to see the code for Support Vector Machines </a></h6>
   

</section>

                  <header>
                    <h4 class="h4 article-title">Results</h4>
                  </header>
          
                 
                  <section class="about-text">
                    <p>
                      
                      
                    <b>Linear SVM:</b>   <br>
          
                      <img class="proImg" src="projectImages/svmOutput1.png" alt="Final data">
                      The Linear SVM achieves an accuracy of 84.90%, showing a competent fit to the data. It evenly distributes its errors between false positives and false negatives, signaling a balanced error response but pointing to areas for improvement to boost both precision and recall. Despite its solid performance, the model suggests potential for enhancing its classification boundaries to better manage the data complexities.
                      <br>
                      <b>RBF Kernel SVM:</b> <br>

                      <img class="proImg" src="projectImages/svmOutput2.png" alt="Final data">
                      The RBF Kernel SVM stands out with an impressive accuracy of 95.61%, demonstrating excellent generalization from the training to the test data. Its minimal false positives and negatives indicate a high capability in capturing complex data patterns, making it the most effective among the evaluated models. This model's strong performance underscores its suitability for complex datasets requiring nuanced data pattern recognition.
                      <br>
                      <b>Polynomial Kernel SVM:</b> <br>
                      <img class="proImg" src="projectImages/svmOutput3.png" alt="Final data">
                      
                      
                      With an accuracy of 93.10%, the Polynomial Kernel SVM is highly effective, especially in managing nonlinear data interactions. Although it records slightly more false positives and negatives than the RBF kernel, it substantially surpasses the Linear SVM, showcasing robust predictive power. This model is particularly useful where data relationships can be approximated by polynomial terms, albeit slightly less optimal than the RBF kernel for this specific dataset. <br>
                      <br>
                      The RBF kernel SVM stands out as the most effective model for this dataset, offering the highest accuracy and the best balance of predictive performance metrics. It is particularly suited for complex datasets where the interaction between features is nonlinear and intricate. The Polynomial SVM provides a valuable alternative with robust performance, especially useful where data relationships can be approximated by polynomial expressions. In contrast, the Linear SVM, while still providing a respectable baseline, shows potential limitations in handling the complexities inherent in the data as effectively as the kernel-based methods. <br>
                      <br>
                    </p>
         
                    
                  </section>


                  <header>
                    <h4 class="h4 article-title">Conclusion</h4>
                  </header>
                  
                  <section class="about-text">
                    <p>
                      The study effectively highlights the potential of sophisticated analytical approaches to predict customer churn, which is crucial for developing strategies to enhance customer retention. The chosen methodology proved successful in identifying patterns and trends that indicate the likelihood of churn, thereby providing actionable insights that can help in formulating targeted interventions to reduce churn rates.<br>
                      <br>


                      Furthermore, the insights gained from this analysis could be instrumental in refining customer engagement and loyalty programs. By understanding the key factors that influence churn, organizations can tailor their customer service and retention strategies more effectively, ultimately leading to improved customer satisfaction and loyalty. This project not only underscores the importance of predictive analytics in strategic decision-making but also demonstrates its practical implications in a business context, particularly in optimizing customer relationship management.<br>

                      <br>
                  
                    </p>
                          
                    </section>  
            </li>
            
            <li class="project-item  active" data-filter-item data-category="clustering">
                <header>
                    <h2 class="h2 article-title">Clustering</h2>
                  </header>
          
                  <header>
                    <h4 class="h4 article-title">Overview</h4>
                  </header>
          
                  <section class="about-text">
                    
                    <p>
                      <b>What is Clustering?</b>
                    </p>
                    <p>
                      Clustering represents an unsupervised learning technique utilized in machine learning and data mining. This method categorizes a collection of items into clusters, ensuring that items within the same cluster exhibit higher similarity amongst themselves compared to items in different clusters. The primary objective of clustering is to uncover underlying patterns or structures present within the data.
                    </p>
          
                    <p>
                      <b>Clustering can be broadly categorized into two principal types:</b>
                    </p>
                    <p>
                      <ol>
                        <li>1. Partitional Clustering </li>
                        <li>2. Hierarchical Clustering</li>
                      </ol>
                    </p>
                    <img class="proImg" src="projectImages/kmeansIntro.png" alt="Final Data">
                    <p>

                      <b>Partitional Clustering:</b> <br>

                      Partitional clustering segregates the dataset into exclusive subsets, ensuring each data point is assigned to a singular subset. A prevalent technique within this category is K-Means Clustering. <br>
                      <b>K-Means Clustering:</b><br>
                      
                      - This method seeks to divide 'n' observations into 'k' clusters, positioning each observation within the cluster whose mean is nearest. It predominantly utilizes Euclidean distance for measurement. Through iterative processes, it reassigns each data point to one of the 'k' clusters based on the provided attributes. <br>
                      
                      - Determining the optimal cluster count, 'k', can be achieved using several methods, including the Silhouette Method. This technique evaluates the similarity of an object to its cluster (cohesion) against its similarity to other clusters (separation), with values ranging from +1 to -1. A higher silhouette value signifies a well-integrated object within its cluster and distinct from other clusters. <br>
                      <br>
                   
                      <b>Hierarchical Clustering:</b> <br>
                      
                      - Hierarchical clustering constructs a cluster hierarchy, or tree. It not only groups data points but also arranges clusters in a hierarchical order, visualized through a dendrogram which encapsulates detailed data insights. <br>
                      
                      - <b>Cosine Similarity in Hierarchical Clustering</b> <br>
                      
                      This clustering can employ various distance metrics, such as Cosine Similarity, which calculates the cosine angle between two vectors. This metric is particularly effective for high-dimensional data, like text analysis, by focusing on data vector orientations rather than their positional distances. <br>
                      <br>
                      <b>K-Means vs. Hierarchical Clustering Comparison:</b> <br>
                      
                      - <b>K-Means Clustering</b> operates by partitioning the dataset into distinct, non-overlapping clusters with iterative centroid adjustments to minimize intra-cluster variance. It requires pre-specification of 'k', the cluster count, and is ideal for identifying spherical clusters due to its reliance on distance measures. <br>
                      
                      - <b>Hierarchical Clustering</b>, in contrast, forms a cluster tree without a predetermined cluster count, allowing for any number of clusters based on desired granularity. It can capture more complex cluster shapes beyond spherical, offering flexibility in cluster formation. <br>
                      <br>
                      <b>Visualization Differences:</b> <br>
                      
                      - <b>K-Means Clustering</b> is typically visualized through scatter plots, with data points colored based on cluster membership, highlighting the spatial distribution of clusters. <br>
                      
                      - <b>Hierarchical Clustering</b> is represented by dendrograms, illustrating the nested clustering and similarity levels, where branch heights indicate cluster similarities, offering a comprehensive view of data grouping. <br>
                    </p>
                    <img class="proImg" src="projectImages/clusteringImage.webp" alt="Differences between Clustering">
                    <p style="text-align: center;">Figure illustrates the visual distinctions between K-Means and Hierarchical Clustering</p>

                    <p>
                      <b>Distance Metrics in Clustering:</b> <br>

                      Distance metrics play a crucial role in various machine learning algorithms, particularly in clustering. They measure the similarity or dissimilarity between data points. Common distance metrics employed in clustering include: <br>
                      
                      - Euclidean Distance: Measures the straight-line distance between two points in Euclidean space. <br>
                      - Manhattan Distance: Calculates the distance between two points by summing the absolute differences of their coordinates. <br>
                      - Minkowski Distance: A generalization of Euclidean and Manhattan distances, adjustable according to a parameter. <br>
                      - Cosine Similarity: Assesses the cosine of the angle between two vectors, indicating directional similarity regardless of magnitude. <br>
                      - Pearson Correlation Distance: Reflects the linear correlation between two variables, indicating how closely data points follow a linear trend. <br>
                      - Spearman Correlation Distance: Measures the monotonic relationship between two variables, based on rank correlations. <br>
                      - Kendall Correlation Distance: Evaluates the ordinal association between two measured quantities. <br>
                      <br>
                      Selecting an appropriate distance metric is pivotal in clustering, as it determines the calculation of similarity between elements and significantly affects the resultant cluster shapes. <br>
 
                      - The selection of the Euclidean distance metric for this project was driven by its suitability to the dataset's characteristics and the analytical methods applied, namely PCA for dimensionality reduction and K-means for clustering. This approach highlights the importance of choosing a distance metric that complements the data's nature and the analysis objectives, ensuring that the clustering results are both statistically robust and insightful. Through this methodical selection, the analysis achieves a balance of computational efficiency and interpretability, providing a solid foundation for deriving meaningful insights from the clustering process. <br>
                    </p>
                  </section>

                  <header>
                    <h4 class="h4 article-title">Data</h4>
                  </header>
          
                  <section class="about-text">
                    <p>
                      <b>Data Preparation in Machine Learning:</b> <br>

                      Data preparation is an essential phase in any machine learning endeavor. It entails converting raw data into a structure that machine learning models can effortlessly interpret and leverage. Specifically, for clusteringâ€”a form of unsupervised learningâ€”the data needs to be both unlabeled and numerical. Clustering algorithms categorize similar items by evaluating their proximity within the feature space, necessitating numerical inputs for accurate distance computation. <br>
                    </p>
          
                    <p>
                      <b>Main Steps in Data Preparation for Clustering Analysis</b> <br>

                      1. Label Removal for Unsupervised Learning <br>
                      2. Dataset Standardization for Equal Contribution <br>
                      3. Dimensionality Reduction via Principal Component Analysis <br>
                      <br>
                     - In the conducted project, a structured data preparation process was essential to ensure the effectiveness of the K-means clustering analysis. This preparation involved several critical steps tailored to optimize the dataset for clustering, enhance the interpretability of the results, and ensure computational efficiency. The following paragraphs outline these steps and their importance in the analytical process. <br>
                     <img class="proImg" src="projectImages/initData.png" alt="intial data">
                     <p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>
                     - Initially, the dataset underwent preprocessing to refine its structure for analysis. This included the removal of the 'Attrition_Flag' column, which served as the label in the original dataset. By excluding this label, the analysis shifted from a supervised to an unsupervised learning approach, focusing purely on discovering inherent patterns and groupings within the data without any preconceived categorizations. Additionally, to address potential biases and scale disparities among the features, the dataset was standardized. Standardization involved scaling the dataset to have a mean of zero and a standard deviation of one for each feature. This step was crucial for ensuring that all variables contributed equally to the analysis, preventing features with larger scales from disproportionately influencing the distance calculations fundamental to K-means clustering. <br>

                     - Following the initial preprocessing, Principal Component Analysis (PCA) was employed as a dimensionality reduction technique. PCA served to condense the information contained across multiple original features into a smaller set of new, uncorrelated variables known as principal components. This reduction was aimed at retaining the most significant variance present in the data while minimizing information loss. By transforming the dataset into a lower-dimensional space, PCA not only facilitated a more computationally efficient clustering process but also enhanced the interpretability of the results. The principal components provided a simplified yet informative representation of the dataset, enabling the identification of patterns and clusters that might not have been apparent in the high-dimensional original feature space. <br>
                     - The combination of these data preparation stepsâ€”removing labels, standardizing the dataset, and applying PCA for dimensionality reductionâ€”was instrumental in laying a robust foundation for the K-means clustering analysis. By meticulously preparing the data, the project ensured that the clustering process was both meaningful and optimized, capable of uncovering and visualizing the natural groupings within the dataset. This approach highlights the significance of thorough data preparation in unlocking the full potential of machine learning algorithms to derive actionable insights from complex datasets. <br>
                     <img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
                     <img class="proImg" src="projectImages/pcaData.png" alt="Final data">
                     <p style="text-align: center;">The above is the sample of the data, having undergone preprocessing steps as described earlier, is now prepared and will be used for subsequent clustering analysis.</p>

                    </p>
                  </section>

                  <header>
                    <h4 class="h4 article-title">Code</h4>
                  </header>
          
                  <section class="about-text">
                    <p>
                      K-means clustering has been implemented using Python, while hierarchical clustering has been executed with R, utilizing the hclust function specifically. Sample data and code for both implementations are available at the following link.
                    </p>
          

                      <h6 class="h6"><a href="https://github.com/kundansaichowdary/Advanced-Banking-Customer-Churn-Prediction/tree/main/Module-2" target="_blank">Click here to see the codes for Clustering </a></h6>
                     

                  </section>

                  <header>
                    <h4 class="h4 article-title">Results</h4>
                  </header>
          
                  <section class="about-text">
                    <p>
                      


                      This analysis embarked on a detailed examination of the dataset through the lens of unsupervised learning, employing both K-means and hierarchical clustering techniques to uncover inherent groupings within the data. The preparatory phase involved standardizing the dataset and applying Principal Component Analysis (PCA), reducing dimensionality while retaining significant variance. This step was crucial for enhancing clustering efficacy and interpretability. <br>
                      
                      <b>- K-means Clustering Analysis:</b> <br>
                      <img class="proImg" src="projectImages/silhouetteOutput.png" alt="Final Data">
                      Using the Silhouette method to ascertain the optimal number of clusters, the analysis explored values of <b>k=2</b>, <b>k=3</b>, and <b>k=9</b>. The Silhouette scores obtained were instrumental in identifying <b>k=3</b> as the optimal cluster number, achieving the highest score of 0.1725. This finding indicates a superior balance of within-cluster cohesion and between-cluster separation for <b>k=3</b>, suggesting the most meaningful and distinct grouping of data points within the dataset. Visualizations included in the report, such as the silhouette score graph and cluster plots for <b>k=2</b>, <b>k=3</b>, and <b>k=9</b>, vividly illustrate these findings, underscoring the distinctiveness of the grouping achieved with <b>k=3</b>. <br>
                      <img class="proImg" src="projectImages/K3output.png" alt="Final Data">
                      <img class="proImg" src="projectImages/k29output.png" alt="Final Data">
                      <b>Hierarchical Clustering Insights</b> <br>
                      
                      Complementing the K-means analysis, hierarchical clustering was applied, visualized through a dendrogram. This visualization facilitated the examination of data groupings and supported the selection of <b>k=3</b> clusters as a coherent choice, aligning with the K-means findings. The dendrogram serves as a visual testament to the natural groupings within the dataset, further validating the analytical approach and outcomes derived from the K-means clustering. <br>
                      <img class="proImg" src="projectImages/hclustOutput.png" alt="Final Data">
                      <b>Comparative Analysis and Conclusions</b> <br>
                      
                      The juxtaposition of K-means and hierarchical clustering results reveals a consistent narrative: the dataset exhibits a natural division into three distinct clusters. This consistency across clustering techniques not only reinforces the validity of the analytical approach but also highlights the nuanced understanding of the dataset's underlying patterns. The application of PCA prior to clustering emerged as a pivotal step, mitigating the "curse of dimensionality" and enabling a more focused and insightful clustering process. <br>
                      
                      <br>
                      The results from this study underscore the power of combining PCA with advanced clustering techniques to unveil hidden structures within complex datasets. Through methodical analysis and the strategic use of Silhouette scores for selecting <b>k</b>, this investigation provides a statistically robust and interpretable framework for understanding the inherent groupings in the dataset. These findings lay a solid foundation for further exploration, offering valuable insights into the dataset's characteristics and potential applications of the identified clusters. <br>


                    </p>
                 
                  </section>

                  <header>
                    <h4 class="h4 article-title">Conclusion</h4>
                  </header>
          
                  <section class="about-text">
                    <p>
                      The exploration of the dataset through both K-means and Hierarchical Clustering methodologies offered comprehensive insights into its inherent structure and grouping dynamics. K-means clustering, guided by the Silhouette method, revealed that an optimal division into three distinct groups maximized the similarity within clusters and the dissimilarity between clusters. This approach elucidated the dataset's segmentation, allowing for clear differentiation and understanding of underlying patterns. <br>
                      <br>
                      Hierarchical Clustering provided a contrasting perspective by illustrating the dataset's hierarchical structure through a dendrogram. This visualization facilitated an understanding of natural groupings at various levels of granularity, showcasing the depth of the dataset's structure. The application of cosine similarity, especially in a subset analysis, underscored the significance of angle-based similarity measures in capturing high-dimensional data relationships. <br>
                      <br>
                      K-means clustering was noted for its computational efficiency and the ability to clearly partition data into distinct, non-overlapping clusters, suitable for large datasets and straightforward segmentation needs. Conversely, Hierarchical Clustering offered a nuanced portrayal of data relationships, revealing the hierarchical organization of data points and proving invaluable for exploratory analysis when the number of clusters is not predetermined. <br>
                      <br>
                      Both clustering techniques yielded valuable, albeit distinct, insights into the dataset. K-means provided clear, distinct groupings based on predefined criteria, whereas Hierarchical Clustering revealed the data's layered structure for a detailed exploration of intrinsic relationships. The integration of these methods into the analysis pipeline highlighted the multifaceted nature of data and the importance of diverse analytical approaches for comprehensive insights. The choice between these methods, or their combined use, largely depends on the analysis goals, dataset characteristics, and desired result granularity. <br>
                    </p>
                  </section>
            </li>



            <li class="project-item  active" data-filter-item data-category="association rule mining">
              <header>
                  <h2 class="h2 article-title">Association Rule Mining</h2>
                </header>
        
                <header>
                  <h4 class="h4 article-title">Overview</h4>
                </header>
        
                <section class="about-text">
                  
                  <p>
                    Association Rule Mining (ARM) emerges as a powerful technique for uncovering hidden relationships or associations among vast sets of items within large datasets. Predominantly harnessed in market basket analysis, this method plays a pivotal role in deciphering customer purchasing behaviors. By analyzing patterns of product purchases, businesses can identify which items are frequently bought together, offering invaluable insights for cross-selling strategies and inventory management.
                  </p>
                  <p>
                    At the core of ARM are 'if-then' rules, serving as the fundamental structure for establishing associations. The 'if' component, known as the antecedent, specifies the condition under scrutiny, whereas the 'then' component, or the consequent, delineates the outcome observed when the antecedent's conditions are satisfied. This structure not only facilitates a systematic exploration of data for patterns but also aids in the prediction and understanding of consumer behavior, enabling organizations to tailor their offerings more effectively to meet customer needs.
                  </p>
        
                  <p>
                    In the realm of Association Rule Mining (ARM), three pivotal measuresâ€”support, confidence, and liftâ€”play crucial roles in evaluating the strength and relevance of discovered associations between itemsets. Understanding these measures is essential for interpreting the results of ARM analysis effectively.
                  </p>
                  <p>
                    <ol>
                      <li>1. <b>Support</b> illuminates the prevalence of an itemset within the dataset, expressed as a percentage of the total transactions. For instance, a support value of 5% for a particular itemset indicates that this combination of items appears in 5% of all transactions. Support helps in identifying the most common itemsets, thereby focusing on significant patterns.</li>
                      <li>2. <b>Confidence</b> offers insight into the reliability of an inferred association rule. It quantifies the proportion of transactions containing the antecedent that also include the consequent. A higher confidence value suggests a strong belief in the rule's validity, indicating that when the antecedent is purchased, the consequent is likely to be purchased as well.</li>
                      <li>3. <b>Lift</b> goes a step further by comparing the rule's observed confidence with what would be expected if the items were independent of each other. A lift value greater than 1 signals that the rule has a positive effect on the sale of the consequent item, indicating a stronger association beyond mere chance. Conversely, a lift value less than 1 implies that the items are unlikely to be bought together, while a value of 1 suggests no association beyond randomness.</li>
                    </ol>
                  </p>
                  <p>
                    Together, these measures provide a comprehensive toolkit for assessing the strength, reliability, and usefulness of the relationships uncovered through ARM. By carefully analyzing support, confidence, and lift, researchers and practitioners can discern truly valuable associations from spurious ones, guiding strategic decisions in marketing, inventory management, and beyond.
                  </p>
                  <img class="proImg" src="projectImages/armImage.png" alt="Final Data">
                  <p>

                    <b>Apriori Algorithm:</b> <br>

                    - The Apriori algorithm stands out as a cornerstone method within the domain of Association Rule Mining (ARM), renowned for its efficacy in unveiling frequent itemsets within a database. This algorithm employs a systematic "bottom-up" strategy, initially pinpointing individual items that frequently occur across transactions. Subsequently, it incrementally constructs larger itemsets, progressively adding one item at a time through a process termed 'candidate generation'. This iterative expansion continues, provided these emerging itemsets maintain a significant presence within the database. <br>
                   
                    
                    - A distinct feature of the Apriori algorithm is its methodical pruning mechanism, which efficiently eliminates itemsets with insufficient support, thereby streamlining the search space. The iterative cycle of candidate generation and evaluation against the database criteria ensures that only the most relevant and frequently occurring itemsets are considered. The algorithm reaches its conclusion once it can no longer find itemsets that satisfy the minimum support threshold, signifying that all significant associations have been identified. <br>
                    
                    - Through its structured approach, the Apriori algorithm facilitates a thorough exploration of potential item associations, making it an invaluable tool for identifying patterns that can inform strategic business decisions. <br>
                    <br>
                 
                    <b>Here's a step-by-step breakdown of how the algorithm operates:</b> <br>
                   <ol>

                    <li><b>1. Initial Count: </b>Construct a table to tally the support count for each item across the dataset.</li>
                    <li><b>2. Minimum Support Filtering:</b> Items not meeting the minimum support threshold are discarded, ensuring focus on potentially significant itemsets.</li>
                    <li><b>3. Candidate Generation (Join Step):</b> Formulate new candidate itemsets by combining existing itemsets from the previous iteration. This expansion relies on the principle of only extending itemsets that have previously shown to be frequent.</li>
                    <li><b>4. Subset Frequency Check:</b> Evaluate all possible subsets of each new candidate itemset. If any subset is not frequent, the candidate itemset is eliminated from consideration.</li>
                    <li><b>5. Support Count Calculation:</b> Determine the support count for the remaining candidate itemsets by searching the dataset.</li>
                    <li><b>6. Support Comparison:</b> Compare each candidate itemset's support count against the minimum support threshold. Candidates falling short are removed.</li>
                    <li><b>7. Iteration:</b> Continue the process iteratively, generating larger itemsets with each round, until no further expansions produce itemsets meeting the minimum support criteria.</li>
                   </ol> 
                   By progressively narrowing the search space through the Apriori property, the algorithm enhances its efficiency, allowing it to effectively identify meaningful associations within large datasets without exhaustively examining every possible itemset. <br>
            
                  </p>
                  <img class="proImg" src="projectImages/aprioriAlgo.png" alt="Flowchart of Apriori Algorithm">
                  <p style="text-align: center;">Flowchart of Apriori Algorithm</p>


                </section>

                <header>
                  <h4 class="h4 article-title">Data</h4>
                </header>
        
                <section class="about-text">
                  <p>
                    Data Preparation in Machine Learning:

                    Data preparation plays a pivotal role in the success of Association Rule Mining (ARM) tasks, much like in other machine learning domains. For the Apriori algorithmâ€”a fundamental method used in ARMâ€”this involves organizing transactional data in a manner that the algorithm can efficiently process and analyze. Unlike clustering, which requires numerical and unlabeled data, the Apriori algorithm deals with categorical data in the form of itemsets from transactions. Each transaction must be encoded as a set of items, allowing the algorithm to identify and evaluate the frequency of itemsets across the dataset. This step is crucial for the Apriori algorithm to effectively generate candidate itemsets and apply the support threshold, enabling the discovery of meaningful associations between items. The structured preparation of transactional data ensures that the algorithm can accurately uncover these associations, leveraging the inherent patterns within the dataset to inform decision-making processes. <br>
                  </p>
        
                  <p>
                    <b>Main Steps in Data Transformation for Apriori Algorithm:</b> <br>

                    1. Selection of Relevant Attributes <br>
                    2. Transformation into Transactional Format <br>
                    3. Creation of a Simplified Transaction Dataset <br>
                    <br>
                   - To analyze the Bank Churners dataset with the Apriori algorithm for finding association rules among various customer attributes, we first needed to transform the original dataset into a transactional format suitable for this type of analysis. The original dataset contained a wide range of information about bank customers, including demographic data, account characteristics, and credit card details. However, the Apriori algorithm requires data in a specific format where each transaction is a set of items purchased together. <br>
                   <img class="proImg" src="projectImages/initData.png" alt="intial data">
                   <p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>
                   - The focus was on four key attributes that could reveal interesting associations: Education_Level, Marital_Status, Income_Category, and Card_Category. These attributes were chosen because they could potentially exhibit meaningful relationships, such as the correlation between a customer's education level and their chosen card category or income bracket. <br>

                   - To transform the dataset, we first isolated these four columns from the original dataframe. Each row in the transformed dataset represents a "transaction," with the selected attributes for each customer concatenated into a single string. This mimics the format of a market basket transaction, where each item would be a product purchased in a single transaction. In present case, the "items" are the attribute values for each customer. <br>
                   - The transformation process involved iterating over each row of the selected columns, converting the attribute values into a string, and then joining these strings with a comma separator. The result was a series of strings, each representing the combined attributes of a single customer as a pseudo-transaction. This series was then saved to a CSV file without a header and with each pseudo-transaction on a separate line, creating a simplified transaction dataset ready for analysis with the Apriori algorithm. <br>
                   - This transformed dataset, now structured as a series of bank customer "transactions," could be analyzed using the arules package in R to uncover association rules among the selected customer attributes. This approach allows for the exploration of patterns and relationships that might not be immediately apparent from the raw data, offering insights into how different customer demographics and behaviors relate to each other within the bank's clientele. <br>
                   <img class="proImg" src="projectImages/armFinalData.png" alt="Final Data">
                   
                   <p style="text-align: center;">The above is the sample of the data, having undergone preprocessing steps as described earlier, is now prepared and will be used for subsequent Apriori Algorithm analysis.</p>

                  </p>
                </section>

                <header>
                  <h4 class="h4 article-title">Code</h4>
                </header>
        
                <section class="about-text">
                  <p>
                    The implementation of Association Rule Mining (ARM), along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying ARM techniques within a dataset.
                  </p>
        
                    <h6 class="h6"><a href="https://github.com/kundansaichowdary/Advanced-Banking-Customer-Churn-Prediction/tree/main/Module-2">Click here to see the code for Association Rule Mining (ARM) </a></h6>


                </section>

                <header>
                  <h4 class="h4 article-title">Results</h4>
                </header>
        
                <section class="about-text">
                  <p>
                    




The analysis of the Bank Churners dataset through the Apriori algorithm unearthed significant insights, particularly highlighting a robust association between individuals with incomes less than $40K and the "Blue" card classification, with confidence levels surpassing 94%. Marital status also emerged as a pivotal factor, with both married and single individuals showing a high likelihood of falling into the "Blue" category, at 94.11% and 91.91% respectively. This pattern extended to those earning between $40K and $60K, who demonstrated a 93.58% association with the "Blue" classification, underscoring a clear link between income levels, marital status, and card preference. Furthermore, a reciprocal relationship between the "Blue" classification and marital status was identified, indicating a significant intersection of economic characteristics and demographic factors within the dataset. These findings not only reveal the economic conditions prevalent among certain segments but also suggest a deeper, interconnected relationship between income, marital status, and banking preferences, potentially enriched by further analysis of lift values for a more comprehensive understanding of the dataset's dynamics. <br>
<br>
<img class="proImg" src="projectImages/network.png" alt="Final Data">
<b>- Analysis of Top 15 Association Rules (Lift):</b> <br>
<p>
  Examining the top 15 association rules ranked by lift provides further insights into the strength of relationships between different variables. Lift measures the degree of association between antecedent and consequent items, indicating how much more likely the consequent is, given the presence of the antecedent, compared to its expected occurrence. Notably, associations involving the "Blue" product and demographic characteristics such as marital status and income levels feature prominently. For instance, the rule indicating that being single and preferring the "Blue" product is associated with an income of less than $40K demonstrates a lift value of 1.058, indicating a 5.8% increase in the likelihood of this association occurring compared to random chance. Similarly, the association between being single and earning less than $40K yields a lift value of 1.029, suggesting a 2.9% increase in the likelihood of this association. These findings underscore the significance of considering lift values in understanding the strength and relevance of association rules for informed decision-making in marketing strategies and consumer targeting efforts.
</p>
<img class="proImg" src="projectImages/rbl.png" alt="Final Data">
<p style="text-align: center;">Top 15 Rules by Lift</p>
<b>- Analysis of Top 15 Association Rules (Support):</b> <br>
<p>
  The top 15 association rules, sorted by support, offer insights into the prevalence of specific associations within the dataset. Support reflects the relative frequency of an itemset, indicating its significance in the dataset. Notably, associations involving the "Blue" product and demographic attributes such as marital status and income levels dominate this set of rules. For instance, the rule indicating that being married is associated with preferring the "Blue" product boasts the highest support at 50.52%, underscoring the substantial occurrence of this association in the dataset. Similarly, the rule demonstrating that preferring the "Blue" product is associated with earning less than $40K also exhibits high support at 43.92%, indicating its frequent occurrence. These findings highlight the prevalence of specific consumer behavior patterns and underscore the importance of understanding and leveraging these associations in marketing strategies and business decision-making processes.
</p>
<img class="proImg" src="projectImages/rbs.png" alt="Final Data">
<p style="text-align: center;">Top 15 Rules by Support</p>
<b>- Analysis of Top 15 Association Rules (Confidence):</b> <br>
  <p>
    The top 15 association rules, ranked by confidence, shed light on compelling consumer behavior trends and product preferences within the dataset. With confidence serving as a measure of the reliability of the associations, these rules provide valuable insights into the strength of the relationships between various demographic factors and product choices. Notably, individuals earning less than $40K demonstrate a robust preference for the "Blue" product, showcasing confidence levels exceeding 95% across different marital statuses. Moreover, married customers exhibit a notable inclination (94.1% confidence) towards the "Blue" product, while single individuals display a slightly lower yet significant preference (91.9% confidence). Income brackets also emerge as influential factors, with the $40K - $60K range associated with a confidence of 93.6% in selecting the "Blue" product. Bidirectional associations underscore the complex interplay between demographic characteristics and product preferences, underscoring the importance of tailored marketing strategies for enhancing consumer satisfaction and optimizing business outcomes.
  </p>
<img class="proImg" src="projectImages/rbc.png" alt="Final Data">
<p style="text-align: center;">Top 15 Rules by Confidence</p>

                <header>
                  <h4 class="h4 article-title">Conclusion</h4>
                </header>
        
                <section class="about-text">
                  <p>
                    The application of the Apriori algorithm to the Bank Churners dataset, focusing on attributes like Education_Level, Marital_Status, Income_Category, and Card_Category, has revealed significant associations that highlight the influence of socio-economic factors on banking preferences. Notably, a strong link was found between lower income levels, marital status, and preference for the "Blue" card category. This insight underscores the importance of understanding customer demographics to tailor financial products and services effectively. The findings emphasize how marital status, in conjunction with income levels, plays a crucial role in shaping customers' banking choices, offering a nuanced view of the socio-economic dynamics at play. <br>
                    <br>
                    Leveraging association rule mining through the Apriori algorithm has proven to be a powerful approach for uncovering complex relationships within the dataset, providing valuable insights into customer behavior. These insights offer financial institutions a data-driven basis for optimizing product offerings, enhancing marketing strategies, and ultimately improving customer satisfaction and loyalty. The study serves as a testament to the potential of machine learning techniques in extracting meaningful patterns from transactional data, demonstrating their critical role in informed decision-making and strategic planning in the banking sector.
                  </p>
                </section>
          </li>-->

<!-- FCNN -->
          <li class="project-item  active" data-filter-item data-category="fcnn">
            <header>
                <h3 class="h3 article-title">FCNN</h3>
              </header>
      
              <header>
                <h4 class="h4 article-title">Overview</h4>
              </header>
              <section class="about-text">
      
              <p>
                <b>What is Fully Connected Neural Network (FCNN)</b>
              </p>

              <p>


                A Fully Connected Neural Network (FCNN) is a type of artificial neural network where each neuron in one layer is connected to every neuron in the subsequent layer. This dense connectivity allows the model to learn complex, non-linear relationships between features in the dataset. FCNNs are widely used for structured data, classification, regression, and other tasks requiring a global understanding of the input.
<br>
<br>
FCNNs are versatile and widely used for tasks like classification, regression, and general predictions involving structured data. Unlike models designed for spatial or sequential tasks, FCNNs focus on the relationships between features rather than preserving positional or temporal context. This makes them ideal for handling numerical and categorical inputs commonly found in tabular datasets.

<br>
<br>
One of the strengths of FCNNs lies in their flexibility, as they can be easily tailored by adjusting the number of layers, neurons, and activation functions to suit a variety of problems. However, their dense connectivity can also lead to overfitting, especially when working with smaller datasets. To mitigate this, techniques like Dropout, Batch Normalization, and early stopping are commonly employed.
<br>
<br>

In this project, the FCNN acts as a baseline model, leveraging its straightforward architecture to establish initial performance benchmarks. Its simplicity and adaptability provide a strong foundation before transitioning to more advanced models tailored for tabular data.

<br>

              </p>
    
              <img class="proImg" src="projectImages/FCNN.png" alt="FCNN">
              <p style="text-align: center;">Fig.1. Simple Representation of the Fully Connected Neural Network</p>
              <p>




<b>How a Fully Connected Neural Network (FCNN) Works</b> <br>

A Fully Connected Neural Network (FCNN) processes data by passing it through a series of interconnected layers. The journey begins with the input layer, where each neuron corresponds to a feature in the dataset. These neurons simply pass the input data to the next layer without performing computations, serving as the starting point for feature representation.
<br>
<br>

The hidden layers form the core of the FCNN, where most of the computation occurs. Each neuron in a hidden layer computes a weighted sum of the inputs it receives, adds a bias term, and applies an activation function. This operation introduces non-linearity, enabling the network to model complex relationships in the data. Common activation functions include ReLU (Rectified Linear Unit), which mitigates the vanishing gradient problem by allowing gradients to flow through the network effectively, and sigmoid or tanh, which are useful for specific tasks like binary classification or feature scaling.

<br>
<br>
During training, the network learns by minimizing a loss function, which quantifies the difference between the predicted output and the true target. For example, binary classification problems typically use binary cross-entropy, while regression tasks may use mean squared error. The training process involves backpropagation, where the network calculates gradients of the loss function with respect to each weight using the chain rule. These gradients are then used by an optimizer, such as SGD (Stochastic Gradient Descent) or Adam, to update the weights and biases, thereby improving the network's predictions over successive epochs.
<br>
<br>
The final layer, or the output layer, generates predictions. The number of neurons in this layer corresponds to the task at hand: a single neuron with a sigmoid activation for binary classification, multiple neurons with softmax activation for multi-class classification, or a single neuron with linear activation for regression. The output represents the model's interpretation of the input data, transformed through the hierarchical feature representation learned in the hidden layers.
<br>
<br>
An important aspect of FCNNs is their ability to generalize across data. To prevent overfitting, especially when the model is complex or the dataset is small, techniques like Dropout (randomly deactivating neurons during training) and Batch Normalization (stabilizing input distributions across layers) are applied. Additionally, hyperparameter tuning, such as adjusting learning rates, number of layers, and neurons, plays a critical role in optimizing the network's performance.
Overall, the FCNN operates as a pipeline that progressively transforms raw input data into increasingly abstract representations, ultimately producing predictions. Its dense connectivity and hierarchical structure allow it to model intricate patterns, making it a robust choice for a variety of supervised learning tasks.

<br>
<br>

<b>Applications of Fully Connected Neural Networks (FCNNs):  </b> <br>


<b>1. Credit Scoring in Banking:  </b> <br>

Banks and financial institutions use FCNNs to predict the likelihood of loan default or credit card delinquency. By analyzing a combination of numerical (income, credit history) and categorical (employment status, loan type) data, FCNNs identify patterns that help determine creditworthiness. Their ability to handle complex relationships between features allows for more accurate risk assessments.
<br>

<b>2. Fraud Detection in E-Commerce: </b> <br>

E-commerce platforms employ FCNNs to detect fraudulent transactions by analyzing features like transaction amount, location, and device information. FCNNs learn subtle correlations between these variables, enabling the identification of anomalies indicative of fraud. This helps in minimizing financial losses while ensuring a smooth customer experience.
<br>

<b>3. Demand Forecasting in Retail:  </b> <br>

Retail businesses use FCNNs to forecast product demand by analyzing sales history, seasonal trends, and external factors like holidays or promotions. Accurate predictions help optimize inventory management, reduce waste, and ensure timely restocking of products.
<br>
<b>4. Predictive Maintenance in Manufacturing:  </b> <br>

Manufacturing companies leverage FCNNs to predict equipment failures by analyzing sensor data, machine logs, and operating conditions. This enables proactive maintenance, reducing downtime and increasing operational efficiency, which directly impacts profitability.
<br>
<b>5. Disease Diagnosis in Healthcare:  </b> <br>
FCNNs assist in diagnosing diseases by analyzing structured patient data, including lab results, symptoms, and medical history. For example, predicting diabetes or heart disease risk based on patient records. Their ability to model complex patterns in the data helps doctors make informed decisions quickly.
<br>
These real-world applications highlight the versatility of FCNNs across diverse industries, showcasing their potential to drive impactful solutions to complex problems.
<br>

</section>

<header>
<h4 class="h4 article-title">Data</h4>
</header>

<section class="about-text">
<p>
<b>Data Preparation:</b> <br>

Data preparation is a foundational step in the machine learning pipeline, particularly for Fully Connected Neural Networks (FCNNs), which rely on numerical representations of features to effectively learn patterns. This stage involves preprocessing raw data into a format suitable for the network to process, optimize, and make predictions. While FCNNs are versatile in handling a wide variety of structured data, careful preparation, such as normalization of numerical features and encoding of categorical variables, ensures that the model performs optimally. Proper data preparation not only accelerates the training process but also enhances the generalization capability of FCNNs, leading to improved accuracy and robustness in predictions.
<br>
<br>

<img class="proImg" src="projectImages/initData.png" alt="intial data"> 
<p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>

The data preparation for Fully Connected Neural Networks encompasses several key activities: <br>

</p>

<p>

Throughout the data preparation phase, several crucial steps were undertaken to ensure the dataset was primed for the analytical objectives set forth. Initial efforts were directed towards data cleaning, where inconsistencies and inaccuracies were rectified, alongside the removal of duplicate entries. A significant portion of this phase was dedicated to standardization, a process designed to mitigate potential biases and disparities in scale among the dataset's features. By scaling each feature to have a mean of zero and a standard deviation of one, a level playing field was established, ensuring equitable contribution of variables to the analytical process. <br>
<br>
Further, the dataset underwent encoding procedures to facilitate the incorporation of categorical data into the modeling process. Label encoding was applied to ordinal variables, including 'Education_Level' and 'Income_Category', translating these into a numerical format that preserves order. For nominal variables such as 'Gender', 'Marital_Status', and 'Card_Category', one-hot encoding was utilized, expanding these categories into separate binary features, thus avoiding any imposition of artificial ordinality.<br>
<br>
The challenge of an imbalanced class distribution within the "Attrition_Flag" column necessitated additional interventions. The disproportionate representation of "Existing Customer" instances over "Attrited Customer" instances presented a risk of model bias, potentially undermining the model's predictive accuracy for the minority class. To counteract this, Synthetic Minority Over-sampling Technique (SMOTE) was employed, generating synthetic instances of the underrepresented class, thus achieving a balanced class distribution with 8500 samples per class, elevating the dataset's total to 17000 samples.<br>
<br>
Subsequent to addressing the imbalance, the dataset was partitioned into training and testing sets, adhering to an 80:20 ratio. This division was instrumental in allocating a substantial portion of data for model training while reserving a significant fraction for the impartial evaluation of model performance. <br>
<br>
An integral component of the feature selection process involved the identification and elimination of redundant features, with 'CLIENTNUM' being a prime example. Its removal was predicated on its lack of relevance to the patterns of interest, thereby enhancing the model's generalizability and reducing the likelihood of overfitting. <br>
<br>
The feature selection phase employed a tripartite methodology, incorporating Random Forest for determining feature importance, Recursive Feature Elimination (RFE) for its systematic reduction of the feature space, and correlation analysis with the target variable for its efficiency in identifying linear relationships. This comprehensive approach culminated in the distillation of two distinct feature sets: Set 1, comprising features of mutual significance across all three methods, and Set 2, encompassing features unanimously recognized as pivotal in the context of customer churn prediction. This dual-faceted strategy ensured the retention of features that are intrinsically relevant to the target variable, spanning both linear and nonlinear dimensions. <br>
<img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
<b> Features Set-1 :</b> <br>
<img class="proImg" src="projectImages/set1.png" alt="Final data">
<b> Features Set-2 :</b> <br>
<img class="proImg" src="projectImages/set2.png" alt="Final data">
<p style="text-align: center;">The above are the sample of the data, having undergone preprocessing steps as described earlier, are now prepared and will be used for FCNN.</p>

</p>
<p>

<b>Creating and Understanding the Importance of Disjoint Test-Train Splits </b> <br>

<br>
In the construction of predictive models, the partitioning of data into training and testing subsets is an indispensable step. For this analysis, an 70:30 split ratio was employed, allocating 70% of the data for training and the remaining 30% for testing. This division was methodically executed to ensure a substantial volume of data for the model to learn from, while still preserving a considerable portion for validation purposes. The training set functions as the foundational dataset upon which the model is built and refined, allowing the learning algorithms to discern and assimilate the intricate relationships between the features and the target variable. <br>
<br>
The importance of creating a disjoint split between the training and testing sets lies in its capacity to provide an objective evaluation of the model's performance. A disjoint split guarantees that the testing set is a collection of observations that the model has not previously encountered during the training phase. This prevents the model from simply memorizing specific data pointsâ€”a phenomenon known as overfittingâ€”and instead encourages the development of generalization capabilities. By evaluating the model on the testing set, we gain insight into how well it can apply its learned patterns to new, unseen data, which is a robust indicator of its potential efficacy in real-world applications. <br>
<br>
</p>

<b>Training Data</b> <br>
<img class="proImg" src="projectImages/trainData.png" alt="Final data">

<b>Testing data</b> <br>
<img class="proImg" src="projectImages/testData.png" alt="Final data">
</section>

<header>
<h4 class="h4 article-title">Code</h4>
</header>

<section class="about-text">
<p>
The implementation of Fully Connected Neural Networks, along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying Fully Connected Neural Networks.
</p>


<h6 class="h6"><a href="https://colab.research.google.com/drive/1UxIDuTZmPuWljqjAIR_DN_mf8cnUxZvT?usp=sharing" target="_blank">Click here to see the code for Fully Connected Neural Networks </a></h6>


</section>

              <header>
                <h4 class="h4 article-title">Results</h4>
              </header>
      
             
              <section class="about-text">
                <p>
                  The performance of the Fully Connected Neural Network (FCNN) was evaluated using multiple metrics, including accuracy, precision, recall, F1-score, and AUC-ROC. Additionally, the training and validation loss/accuracy trends and the confusion matrix were analyzed to understand the network's learning behavior and predictive capabilities.                  <br>
                  
                  <b>Training and Validation Trends:</b> <br>
                  
                  <b>Accuracy</b> <br>
                  <img class="proImg" src="projectImages/TvaFCNN.png" alt="Training and Validation Accuracy">
                  
                  <b>Loss</b> <br>
                  <img class="proImg" src="projectImages/TvlFCNN.png" alt="Training and Validation Loss">
                  Over the course of 20 epochs, the FCNN demonstrated consistent improvements in both training and validation metrics:                  <br>
                  - Training Accuracy steadily increased from an initial 71.30% to 92.86%, showcasing the model's ability to progressively learn meaningful patterns from the training data.
                  <br>
                  - Validation Accuracy improved from 86.47% in the first epoch to 95.45% by the final epoch, indicating strong generalization capabilities and minimal overfitting. 
                  <br>
                  - Training Loss decreased from 0.6645 to 0.2132, while Validation Loss dropped from 0.4193 to 0.1486, reflecting the network's ability to optimize its weights effectively during training. 
                  <br>
                  The convergence of training and validation losses suggests that the model is neither underfitting nor overfitting, which is an essential indicator of a well-regularized and balanced learning process.
                  <br>
                  
                
                  
                  <b>Confusion Matrix Analysis:</b>
                  
                  <img class="proImg" src="projectImages/CmFCNN.png" alt="Confusion Matrix">
                

                  The confusion matrix provides further insight into the model's classification performance:
                  - True Positives (1,212) and True Negatives (1,243) indicate that the majority of the positive and negative samples were correctly classified.
                  <br>
                  - False Positives (32) and False Negatives (63) were minimal, reflecting the model's high precision (97.42%) and recall (95.05%).
                  <br>
                  The balanced distribution of errors highlights the model's robustness in handling both classes effectively, making it a reliable tool for practical deployment.                  <br>
                  <br>
              
                  <b>Evaluation Metrics: </b> <br>
                  The FCNN achieved exceptional results on the test dataset:
                  <br>
                  - Test Accuracy: 96.27%, confirming the model's high predictive reliability across unseen data.<br>
                  - Precision: 97.42%, demonstrating its ability to avoid false positives, which is critical in contexts where incorrect classifications of positive instances have significant implications. <br>
                  - Recall: 95.05%, signifying its ability to detect nearly all positive instances. <br>
                  - F1-Score: 96.22%, balancing precision and recall to provide an overall measure of model effectiveness. <br>
                  - ROC AUC: 99.22%, reflecting the model's outstanding capability to differentiate between positive and negative classes. <br>
                  <br>
                  These results validate the effectiveness of the FCNN architecture for the given tabular dataset. The combination of steadily decreasing losses, increasing accuracy, and high evaluation metrics underscores the network's ability to extract meaningful patterns and make reliable predictions. The low error rates, coupled with a high AUC-ROC, demonstrate the model's utility in real-world applications where predictive accuracy and robustness are paramount.                  <br>
                  <br>
                  
              </section>


              <header>
                <h4 class="h4 article-title">Conclusion</h4>
              </header>
              
              <section class="about-text">
                <p>
                  
                  The Fully Connected Neural Network (FCNN) has demonstrated exceptional performance in addressing the challenges of our tabular dataset. Through rigorous training and validation, the model achieved a high level of accuracy (96.31%) and robustness, as evidenced by its precision, recall, and AUC-ROC scores. The consistent reduction in training and validation loss, alongside steadily improving accuracy, underscores the model's capacity to generalize effectively across unseen data while avoiding overfitting. These results confirm the suitability of the FCNN for structured data analysis, where it effectively captures complex relationships between features.
                  <br>
                  <br>
                  The comprehensive evaluation, including the confusion matrix and performance metrics, highlights the FCNNâ€™s reliability in minimizing misclassifications while maintaining a balance between precision and recall. This balance is particularly critical in real-world scenarios where errors can have significant implications. The model's ability to optimize its learning process and extract meaningful patterns has set a strong precedent for exploring and comparing more advanced deep learning architectures tailored for tabular data.
                  <br>
                  <br>
                  The FCNN serves as a solid baseline model for this project, offering insights into the dataset's structure and the potential for further improvement. Its performance provides a benchmark for evaluating other architectures, such as Wide & Deep Models and TabNet, paving the way for more advanced methods to enhance predictive power and interpretability. This work demonstrates the versatility and effectiveness of FCNNs in structured data applications, establishing a foundation for deeper exploration and innovation in the machine learning pipeline.
                  <br>
                  <br>
              
                </p>
                      
                </section>  
        </li>


<!-- Wide-Deep Networks -->


        <li class="project-item  active" data-filter-item data-category="wide - deep models">
          <header>
              <h3 class="h3 article-title">Wide Deep Models</h3>
            </header>
            <header>
              <h4 class="h4 article-title">Overview</h4>
            </header>
            <section class="about-text">
    
            <p>
              <b>What is Wide & Deep Models?</b>
            </p>

            <p>


              Wide & Deep Models are hybrid neural network architectures that integrate the strengths of linear models and deep learning. This unique combination allows the model to capture both memorization and generalization simultaneously. Memorization is achieved through the wide component, which explicitly captures feature interactions using a linear model, often leveraging cross-product transformations or manually engineered features. This makes it highly effective for handling sparse or categorical data, identifying specific patterns, and preserving rules like "if-then" relationships. On the other hand, the deep component is a fully connected neural network that learns complex, non-linear relationships between features through multiple hidden layers. It generalizes well by leveraging dense feature embeddings and hierarchical learning, uncovering intricate patterns in the data.
              <br>
<br>
Initially proposed by Google for recommendation systems, Wide & Deep Models have since become a versatile tool across many structured data applications. They excel in tasks like personalized recommendations, search ranking, and predictive analytics, where both explicit feature interactions and complex hidden patterns are essential. By combining these two complementary components, Wide & Deep Models serve the dual purpose of preserving past knowledge (wide) while adapting to new trends and patterns (deep), making them an effective solution for many large-scale, structured data problems.
<br>

<br>
In this project, the Wide & Deep Model serves as a hybrid approach, combining the interpretability of linear models with the representational power of deep learning. Its ability to simultaneously capture explicit feature interactions and complex non-linear patterns makes it an ideal progression from baseline models like the FCNN, offering enhanced performance tailored for structured data.

<br>

            </p>
  
            <img class="proImg" src="projectImages/WDModels.png" alt="Wide Deep Models">
            <!-- <p style="text-align: center;">Fig.1. Simple Representation of the Wide Deep Models</p> -->
            <p>




<b>How Wide & Deep Models Work</b> <br>

Wide & Deep Models operate by seamlessly integrating two distinct but complementary components: the wide component and the deep component. These components are designed to address different aspects of learning from structured data, combining their strengths to create a robust hybrid architecture. The fundamental idea is to simultaneously leverage the ability of linear models to memorize explicit feature interactions and the capacity of deep neural networks to generalize from data by uncovering intricate, non-linear relationships.
<br>
<br>


The wide component functions as a linear model that captures memorization. This component excels at identifying explicit, human-engineered feature interactions or learned cross-product transformations between input features. For example, in a recommendation system, interactions like "user purchased product X after viewing product Y" can be directly modeled in the wide component. The weights associated with these feature interactions are trained using gradient descent, just like in traditional linear regression or logistic regression models. By preserving these specific patterns, the wide component ensures the model retains prior knowledge and addresses sparse data effectively, making it particularly useful for categorical data and high-dimensional inputs.
<br>
<br>
The deep component, on the other hand, is a fully connected neural network that learns complex, non-linear interactions between features. It begins by embedding sparse features into dense vector representations. These embeddings are then passed through multiple hidden layers, where each layer transforms the input using weighted sums, bias terms, and non-linear activation functions such as ReLU. These transformations allow the model to build hierarchical representations of the data, uncovering deeper patterns that the wide component may overlook. The deep component excels in generalization, enabling the model to learn patterns that extend beyond explicitly defined interactions.
<br>
<br>
Both components are trained simultaneously, and their outputs are concatenated to produce a unified prediction. The final prediction layer combines the memorization power of the wide component with the generalization strength of the deep component. During training, the gradient of the loss function propagates through both components, allowing them to optimize together. Loss functions like binary cross-entropy or mean squared error (depending on the task) are typically used, and optimizers like Adam or SGD ensure efficient convergence.
<br>
<br>
An essential advantage of Wide & Deep Models is their flexibility. By incorporating the wide component, the model retains the ability to handle sparse data and explicit rules, while the deep component allows it to learn implicit, non-linear relationships. This combination makes the model versatile and effective for a wide range of tasks, including recommendation systems, search ranking, and predictive analytics. Moreover, techniques like feature embedding, dropout, and batch normalization in the deep component help enhance the modelâ€™s robustness and scalability.
<br>
<br>
Wide & Deep Models function as a bridge between traditional linear models and deep learning, combining the strengths of both paradigms. This hybrid approach ensures that the model is not only capable of capturing specific, predefined patterns in the data but also learning complex and generalized representations, making it a powerful tool for structured data problems.

<br>
<br>

<b>Applications of Wide & Deep Models:  </b> <br>


<b>1. Recommendation Systems:  </b> <br>

Wide & Deep Models are extensively used in recommendation systems to enhance personalization. The wide component captures historical user interactions (e.g., "users who bought X also bought Y"), while the deep component generalizes preferences to suggest new items based on user profiles and behaviors. This hybrid approach powers platforms like Google Play, Amazon, and Netflix by offering accurate and diverse recommendations.

<br>

<b>2. Search Ranking: </b> <br>

Search engines and e-commerce platforms use Wide & Deep Models to improve the relevance of search results. The wide component identifies exact matches or user-query interactions, while the deep component analyzes user intent and contextual semantics. This combination ensures high-ranking, meaningful results tailored to user preferences.
<br>

<b>3. Predictive Analytics:  </b> <br>

In industries like finance and healthcare, Wide & Deep Models predict outcomes such as customer churn, credit risk, or patient readmission. The wide component incorporates explicit business rules or historical patterns, while the deep component models hidden relationships in structured data, enabling accurate predictions and actionable insights.
<br>
<b>4. Dynamic Pricing:  </b> <br>

In industries like travel and e-commerce, Wide & Deep Models assist in setting dynamic prices. The wide component memorizes pricing strategies based on historical data, while the deep component learns relationships between factors like demand, time, and competition. This enables companies to optimize pricing in real-time.
<br>
<b>5. Personalized Learning:  </b> <br>
In education technology, Wide & Deep Models recommend personalized learning paths for students by combining historical performance metrics (wide component) with general learning patterns (deep component). This ensures adaptive learning experiences tailored to individual needs.
<br>
<br>
Wide & Deep Models are versatile and powerful, making them a go-to choice for tasks that require a balance between explicit memorization and deep generalization across various industries and applications. Their ability to handle structured data with precision and adaptability makes them indispensable for modern data-driven decision-making.
<br>

</section>

<header>
<h4 class="h4 article-title">Data</h4>
</header>

<section class="about-text">
<p>
<b>Data Preparation:</b> <br>

Data preparation is a critical step in the development of Wide & Deep Models, as their dual architecture relies on both explicit feature interactions and complex non-linear relationships. For the wide component, preprocessing involves encoding categorical variables and generating cross-product transformations or interaction terms, which help the model memorize specific patterns. Simultaneously, the deep component requires embedding categorical variables into dense representations and normalizing numerical features to facilitate effective gradient-based optimization. This meticulous preprocessing ensures that both components operate harmoniously, leveraging their respective strengths to maximize predictive accuracy and robustness. Properly prepared data enables the Wide & Deep Model to excel in capturing both shallow and deep patterns, ultimately enhancing its performance in structured data tasks.
<br>
<br>

<img class="proImg" src="projectImages/initData.png" alt="intial data"> 
<p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>

The data preparation for Wide & Deep Models encompasses several key activities: <br>

</p>

<p>

Throughout the data preparation phase, several crucial steps were undertaken to ensure the dataset was primed for the analytical objectives set forth. Initial efforts were directed towards data cleaning, where inconsistencies and inaccuracies were rectified, alongside the removal of duplicate entries. A significant portion of this phase was dedicated to standardization, a process designed to mitigate potential biases and disparities in scale among the dataset's features. By scaling each feature to have a mean of zero and a standard deviation of one, a level playing field was established, ensuring equitable contribution of variables to the analytical process. <br>
<br>
Further, the dataset underwent encoding procedures to facilitate the incorporation of categorical data into the modeling process. Label encoding was applied to ordinal variables, including 'Education_Level' and 'Income_Category', translating these into a numerical format that preserves order. For nominal variables such as 'Gender', 'Marital_Status', and 'Card_Category', one-hot encoding was utilized, expanding these categories into separate binary features, thus avoiding any imposition of artificial ordinality.<br>
<br>
The challenge of an imbalanced class distribution within the "Attrition_Flag" column necessitated additional interventions. The disproportionate representation of "Existing Customer" instances over "Attrited Customer" instances presented a risk of model bias, potentially undermining the model's predictive accuracy for the minority class. To counteract this, Synthetic Minority Over-sampling Technique (SMOTE) was employed, generating synthetic instances of the underrepresented class, thus achieving a balanced class distribution with 8500 samples per class, elevating the dataset's total to 17000 samples.<br>
<br>
Subsequent to addressing the imbalance, the dataset was partitioned into training and testing sets, adhering to an 80:20 ratio. This division was instrumental in allocating a substantial portion of data for model training while reserving a significant fraction for the impartial evaluation of model performance. <br>
<br>
An integral component of the feature selection process involved the identification and elimination of redundant features, with 'CLIENTNUM' being a prime example. Its removal was predicated on its lack of relevance to the patterns of interest, thereby enhancing the model's generalizability and reducing the likelihood of overfitting. <br>
<br>
The feature selection phase employed a tripartite methodology, incorporating Random Forest for determining feature importance, Recursive Feature Elimination (RFE) for its systematic reduction of the feature space, and correlation analysis with the target variable for its efficiency in identifying linear relationships. This comprehensive approach culminated in the distillation of two distinct feature sets: Set 1, comprising features of mutual significance across all three methods, and Set 2, encompassing features unanimously recognized as pivotal in the context of customer churn prediction. This dual-faceted strategy ensured the retention of features that are intrinsically relevant to the target variable, spanning both linear and nonlinear dimensions. <br>
<img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
<b> Features Set-1 :</b> <br>
<img class="proImg" src="projectImages/set1.png" alt="Final data">
<b> Features Set-2 :</b> <br>
<img class="proImg" src="projectImages/set2.png" alt="Final data">
<p style="text-align: center;">The above are the sample of the data, having undergone preprocessing steps as described earlier, are now prepared and will be used for Wide & Deep Models.</p>

</p>
<p>

<b>Creating and Understanding the Importance of Disjoint Test-Train Splits </b> <br>

<br>
In the construction of predictive models, the partitioning of data into training and testing subsets is an indispensable step. For this analysis, an 70:30 split ratio was employed, allocating 70% of the data for training and the remaining 30% for testing. This division was methodically executed to ensure a substantial volume of data for the model to learn from, while still preserving a considerable portion for validation purposes. The training set functions as the foundational dataset upon which the model is built and refined, allowing the learning algorithms to discern and assimilate the intricate relationships between the features and the target variable. <br>
<br>
The importance of creating a disjoint split between the training and testing sets lies in its capacity to provide an objective evaluation of the model's performance. A disjoint split guarantees that the testing set is a collection of observations that the model has not previously encountered during the training phase. This prevents the model from simply memorizing specific data pointsâ€”a phenomenon known as overfittingâ€”and instead encourages the development of generalization capabilities. By evaluating the model on the testing set, we gain insight into how well it can apply its learned patterns to new, unseen data, which is a robust indicator of its potential efficacy in real-world applications. <br>
<br>
</p>

<b>Training Data</b> <br>
<img class="proImg" src="projectImages/trainData.png" alt="Final data">

<b>Testing data</b> <br>
<img class="proImg" src="projectImages/testData.png" alt="Final data">
</section>

<header>
<h4 class="h4 article-title">Code</h4>
</header>

<section class="about-text">
<p>
The implementation of Wide & Deep Models, along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying Wide & Deep Models.
</p>


<h6 class="h6"><a href="https://colab.research.google.com/drive/1UxIDuTZmPuWljqjAIR_DN_mf8cnUxZvT?usp=sharing" target="_blank">Click here to see the code for Wide & Deep Models </a></h6>


</section>

            <header>
              <h4 class="h4 article-title">Results</h4>
            </header>
    
           
            <section class="about-text">
              <p>
                The Wide & Deep Model, designed to integrate the memorization capabilities of linear models with the generalization power of deep neural networks, was thoroughly evaluated using metrics such as accuracy, precision, recall, F1-score, and AUC-ROC. The training and validation trends, as well as the confusion matrix, were analyzed to assess its performance and learning behavior.                
                
                <br>
                <b>Training and Validation Trends:</b> <br>
                
                <b>Accuracy</b> <br>
                <img class="proImg" src="projectImages/TvaWDN.png" alt="Training and Validation Accuracy">
                
                <b>Loss</b> <br>
                <img class="proImg" src="projectImages/TvlWDN.png" alt="Training and Validation Loss">
                Across 20 epochs, the Wide & Deep Model demonstrated significant improvements in learning and generalization: <br>               
                - Training Accuracy improved from an initial 77.60% in the first epoch to a remarkable 94.86% in the final epoch, showcasing the model's capacity to effectively learn both shallow and deep patterns in the data. 
                <br>
                - Validation Accuracy increased consistently, starting at 87.18% and reaching 95.06% by the end, indicating robust generalization on unseen validation data. 
                <br>
                - Training Loss steadily decreased from 0.4594 to 0.1349, while Validation Loss dropped from 0.3135 to 0.1292, reflecting the model's ability to optimize its parameters effectively and avoid overfitting. 
                <br>
                The close alignment between training and validation loss curves suggests that the model achieved a well-balanced learning process, capable of delivering reliable performance on both seen and unseen data.
                <br>
                
              
                
                <b>Confusion Matrix Analysis:</b>
                
                <img class="proImg" src="projectImages/CmWDN.png" alt="Confusion Matrix">
              

                The confusion matrix provides valuable insights into the classification accuracy of the model:
                - True Positives (1,193) and True Negatives (1,230) highlight the model's capability to accurately classify the majority of samples.
                <br>
                - False Positives (45) and False Negatives (82) were kept to a minimum, indicating a strong ability to minimize both types of classification errors.
                <br>
                These results reflect the model's high precision and recall, further emphasizing its reliability for real-world applications.                  
                <br>
                <br>
            
                <b>Evaluation Metrics: </b> <br>
                The Wide & Deep Model achieved exceptional performance metrics on the test dataset:
                <br>
                - Test Accuracy: 95.02%, indicating the model's high overall classification accuracy on unseen data.<br>
                - Precision: 96.37%, demonstrating its effectiveness in minimizing false positives and reliably identifying positive instances. <br>
                - Recall: 93.57%, reflecting its ability to detect the majority of true positive samples. <br>
                - F1-Score: 94.95%, balancing precision and recall to present a holistic measure of model performance. <br>
                - ROC AUC: 99.03%, showcasing its superior ability to distinguish between classes, even under challenging conditions. <br>
                <br>
                The Wide & Deep Model excelled in both learning and generalization, effectively combining its wide and deep components to handle explicit feature interactions and uncover complex patterns. The steadily increasing accuracy and decreasing losses across training and validation phases underscore the model's robustness and stability. Furthermore, the low number of false positives and false negatives, along with the exceptional AUC-ROC score, demonstrate its reliability for deployment in structured data tasks.
                <br>
                
            </section>


            <header>
              <h4 class="h4 article-title">Conclusion</h4>
            </header>
            
            <section class="about-text">
              <p>
                
                The Wide & Deep Model has demonstrated its strength as a robust architecture capable of bridging the gap between explicit feature memorization and deep pattern generalization. Its hybrid nature, combining the best aspects of linear models and neural networks, allows it to excel in structured data tasks where capturing both shallow interactions and intricate relationships is crucial. The model's superior performance metrics, such as high accuracy, precision, recall, and AUC-ROC, reinforce its effectiveness in leveraging both wide and deep components to achieve predictive excellence.                <br>
                <br>
                The consistently decreasing training and validation losses, coupled with the balanced improvements in accuracy, highlight the model's well-tuned optimization process. The low number of classification errors, reflected in the confusion matrix, further validates its reliability in handling complex data distributions. Moreover, the strong alignment between training and validation trends underscores its ability to generalize effectively without overfittingâ€”a critical requirement for real-world applications.                <br>
                <br>
                The Wide & Deep Model not only meets but exceeds expectations for predictive accuracy and robustness, establishing itself as a powerful framework for structured data problems. These results lay the foundation for advancing to more sophisticated architectures, such as TabNet, ensuring continuous progress in leveraging cutting-edge machine learning techniques for our task.                <br>
                <br>
            
              </p>
                    
              </section>      

    
            <p>

      </li>

<!-- TabNet -->

      <li class="project-item  active" data-filter-item data-category="tabnet">
        <header>
            <h3 class="h3 article-title">TabNet</h3>
          </header>

          <header>
            <h4 class="h4 article-title">Overview</h4>
          </header>
          <section class="about-text">
  
          <p>
            <b>What is TabNet?</b>
          </p>

          <p>


            TabNet is a cutting-edge deep learning architecture specifically designed for modeling tabular data. Unlike traditional approaches that rely on gradient boosting or shallow neural networks for tabular datasets, TabNet employs an attention-based mechanism to dynamically select which features to focus on at each decision step. This innovative design allows TabNet to capture both global patterns and intricate feature interactions, making it a robust and interpretable model for structured data.
            <br>
<br>
TabNet was introduced by researchers at Google Cloud and is known for combining the strengths of neural networks with inherent interpretabilityâ€”a feature traditionally associated with decision tree-based models. By leveraging a sequential decision-making process and learnable feature masks, TabNet learns which features are most relevant for predictions, providing a built-in mechanism for feature selection and interpretation. This capability makes TabNet a highly versatile and powerful model for a wide range of tasks, including classification, regression, and anomaly detection.
<br>
<br>
One of the strengths of FCNNs lies in their flexibility, as they can be easily tailored by adjusting the number of layers, neurons, and activation functions to suit a variety of problems. However, their dense connectivity can also lead to overfitting, especially when working with smaller datasets. To mitigate this, techniques like Dropout, Batch Normalization, and early stopping are commonly employed.
<br>
<br>

In this project, TabNet serves as an advanced architecture specifically designed for tabular data, leveraging its unique attention-based mechanism to dynamically select and prioritize features. Its ability to balance performance and interpretability makes it a natural progression from simpler models like the FCNN and Wide & Deep Models. By capturing both global patterns and intricate feature interactions, TabNet provides a powerful framework for extracting deep insights and achieving superior predictive accuracy on structured datasets.
<br>

          </p>

          <img class="proImg" src="projectImages/TabNet.png" alt="TabNet">
          <p style="text-align: center;">Fig.1. Simple Representation of the TabNet</p>
          <p>




<b>How TabNet Works</b> <br>

TabNet employs an innovative attention-based mechanism to process tabular data, effectively addressing the challenges associated with feature selection, interpretability, and generalization. Its architecture is built around a sequential decision-making process, where the model dynamically determines which features to focus on at each step, making it uniquely suited for structured datasets.
<br>
<br>

<b>Input Representation</b> <br>
TabNet starts with raw tabular data, where features can be both categorical and numerical. Numerical features are typically normalized, while categorical features are embedded into dense representations. These processed inputs are then passed into the network, which treats them as a sequential decision process rather than processing them all at once.
<br>
<br>

<b>Feature Attention Mechanism</b> <br>
At the heart of TabNet is its feature attention mechanism, which utilizes learnable masks to determine the importance of each feature at every step. This attention mechanism is implemented through an Attentive Transformer, which dynamically selects subsets of features to focus on, while other features are deferred for later decision steps. This dynamic selection ensures that the model efficiently handles high-dimensional data without overwhelming the network with irrelevant information.
<br>
<br>

<b>Sequential Decision Process</b> <br>
TabNet consists of a series of decision steps, each comprising two main components:
<br>

<ol>
  <li>
    1. Feature Transformer: This is a fully connected neural network that applies nonlinear transformations to the selected features, capturing complex interactions and patterns.
  </li>
  <li>
    2. Attentive Transformer: After transforming the features, the Attentive Transformer generates a new feature mask for the next step, ensuring that only the most relevant features are passed forward.

  </li>
</ol>
This iterative process allows the model to progressively refine its focus on critical features, uncovering intricate relationships and hierarchies within the data.
<br>
<br>

<b>Sparse Feature Selection</b> <br>

One of TabNet's defining characteristics is its use of sparse feature selection, where only a subset of features is actively considered at each step. This sparsity is achieved through a learnable penalty term that encourages the model to use fewer features, improving both interpretability and generalization.
<br>
<br>

<b>Output Prediction</b> <br>
At each decision step, the outputs are aggregated using a learnable mechanism to produce the final prediction. This aggregation ensures that all learned representations from different steps contribute to the prediction, creating a balance between shallow and deep patterns.
<br>
<br>

<b>Self-Interpretability</b> <br>
TabNet is inherently interpretable due to its feature masks, which explicitly indicate the importance of each feature at every step. This interpretability is a significant advantage over traditional neural networks, allowing practitioners to understand and trust the model's decisions.
<br>
<br>

<b>Training with Differentiable Loss</b> <br>
TabNet uses standard gradient-based optimization methods for training. The sparse feature selection and dynamic attention mechanisms are differentiable, ensuring that the network can be trained end-to-end using backpropagation. Regularization techniques like sparsity constraints are integrated into the loss function to encourage efficient feature usage.
<br>

TabNetâ€™s sequential decision-making process, powered by attention-based feature selection, allows it to focus on the most relevant aspects of the data at every step. This unique design not only enhances performance but also provides a transparent and interpretable model, making TabNet an excellent choice for tabular data tasks where both accuracy and interpretability are paramount.
<br>
<br>

<b>Applications of TabNet:  </b> <br>


<b>1. Fraud Detection in Finance:  </b> <br>

TabNet excels in tasks like detecting fraudulent transactions, where patterns in structured tabular data are complex and subtle. Its interpretability enables financial institutions to not only predict fraud with high accuracy but also understand the key features contributing to suspicious activities, aiding compliance and regulatory reporting.

<br>

<b>2. Credit Scoring: </b> <br>

In the domain of credit scoring, TabNet provides transparent predictions by identifying critical factors such as income, credit history, and debt-to-income ratio. This interpretability helps banks and financial institutions make fair and explainable credit decisions while maintaining high accuracy in risk assessment.

<br>

<b>3. Recommendation Systems:  </b> <br>

TabNet is used in recommendation systems to provide personalized product or content suggestions. By combining its ability to handle sparse and dense features, it identifies both specific user preferences and generalized trends, making it effective for e-commerce, streaming platforms, and digital marketing.

<br>
<b>4. Energy Usage Forecasting:  </b> <br>

Utility companies use TabNet to predict energy consumption patterns based on historical usage, weather conditions, and demographic information. Its interpretability helps identify key drivers of energy demand, enabling better resource allocation and planning.

<br>

<b>5. Loan Default Prediction:  </b> <br>

TabNet's capability to handle high-dimensional tabular data makes it ideal for predicting loan defaults. It provides actionable insights by highlighting the most influential factors, such as loan amount, interest rates, and borrower profiles, improving decision-making processes for lenders.
<br>

TabNetâ€™s versatility in handling structured data, coupled with its interpretability and robust performance, makes it a preferred choice for diverse real-world applications. Whether itâ€™s identifying risks, optimizing resources, or enhancing user experiences, TabNet proves invaluable across multiple domains, driving innovation and informed decision-making.
<br>

</section>

<header>
<h4 class="h4 article-title">Data</h4>
</header>

<section class="about-text">
<p>
<b>Data Preparation:</b> <br>

Data preparation is a crucial step when employing TabNet, as its unique architecture is specifically designed to handle raw tabular data without extensive feature engineering. Unlike traditional models, TabNet is capable of learning feature importance dynamically, reducing the need for manual feature interactions. However, ensuring a clean and well-structured dataset remains essential to maximize its performance and interpretability.
<br>
<br>
For numerical features, normalization or scaling is typically recommended to align feature distributions, thereby accelerating convergence during training. Categorical variables can be directly encoded into numerical representations, but TabNetâ€™s inherent capability to handle embeddings often allows for a more efficient representation of these features. Additionally, missing values should be addressed through imputation or specialized handling to avoid disruptions in learning patterns.
<br>
<br>
By ensuring high-quality input data, TabNet is better positioned to leverage its attentive and sequential feature selection mechanisms, leading to enhanced predictive performance. Proper preparation not only aligns the data with TabNetâ€™s learning paradigm but also ensures robustness and accuracy in downstream predictions, making the model well-suited for complex structured datasets.
<br>
<br>

<img class="proImg" src="projectImages/initData.png" alt="intial data"> 
<p style="text-align: center;">The above is the sample of the data before pre-processing i.e., the raw data</p>

The data preparation for TabNet encompasses several key activities: <br>

</p>

<p>

Throughout the data preparation phase, several crucial steps were undertaken to ensure the dataset was primed for the analytical objectives set forth. Initial efforts were directed towards data cleaning, where inconsistencies and inaccuracies were rectified, alongside the removal of duplicate entries. A significant portion of this phase was dedicated to standardization, a process designed to mitigate potential biases and disparities in scale among the dataset's features. By scaling each feature to have a mean of zero and a standard deviation of one, a level playing field was established, ensuring equitable contribution of variables to the analytical process. <br>
<br>
Further, the dataset underwent encoding procedures to facilitate the incorporation of categorical data into the modeling process. Label encoding was applied to ordinal variables, including 'Education_Level' and 'Income_Category', translating these into a numerical format that preserves order. For nominal variables such as 'Gender', 'Marital_Status', and 'Card_Category', one-hot encoding was utilized, expanding these categories into separate binary features, thus avoiding any imposition of artificial ordinality.<br>
<br>
The challenge of an imbalanced class distribution within the "Attrition_Flag" column necessitated additional interventions. The disproportionate representation of "Existing Customer" instances over "Attrited Customer" instances presented a risk of model bias, potentially undermining the model's predictive accuracy for the minority class. To counteract this, Synthetic Minority Over-sampling Technique (SMOTE) was employed, generating synthetic instances of the underrepresented class, thus achieving a balanced class distribution with 8500 samples per class, elevating the dataset's total to 17000 samples.<br>
<br>
Subsequent to addressing the imbalance, the dataset was partitioned into training and testing sets, adhering to an 80:20 ratio. This division was instrumental in allocating a substantial portion of data for model training while reserving a significant fraction for the impartial evaluation of model performance. <br>
<br>
An integral component of the feature selection process involved the identification and elimination of redundant features, with 'CLIENTNUM' being a prime example. Its removal was predicated on its lack of relevance to the patterns of interest, thereby enhancing the model's generalizability and reducing the likelihood of overfitting. <br>
<br>
The feature selection phase employed a tripartite methodology, incorporating Random Forest for determining feature importance, Recursive Feature Elimination (RFE) for its systematic reduction of the feature space, and correlation analysis with the target variable for its efficiency in identifying linear relationships. This comprehensive approach culminated in the distillation of two distinct feature sets: Set 1, comprising features of mutual significance across all three methods, and Set 2, encompassing features unanimously recognized as pivotal in the context of customer churn prediction. This dual-faceted strategy ensured the retention of features that are intrinsically relevant to the target variable, spanning both linear and nonlinear dimensions. <br>
<img class="proImg" src="projectImages/hcFinal.png" alt="Final Data">
<b> Features Set-1 :</b> <br>
<img class="proImg" src="projectImages/set1.png" alt="Final data">
<b> Features Set-2 :</b> <br>
<img class="proImg" src="projectImages/set2.png" alt="Final data">
<p style="text-align: center;">The above are the sample of the data, having undergone preprocessing steps as described earlier, are now prepared and will be used for TabNet.</p>

</p>
<p>

<b>Creating and Understanding the Importance of Disjoint Test-Train Splits </b> <br>

<br>
In the construction of predictive models, the partitioning of data into training and testing subsets is an indispensable step. For this analysis, an 70:30 split ratio was employed, allocating 70% of the data for training and the remaining 30% for testing. This division was methodically executed to ensure a substantial volume of data for the model to learn from, while still preserving a considerable portion for validation purposes. The training set functions as the foundational dataset upon which the model is built and refined, allowing the learning algorithms to discern and assimilate the intricate relationships between the features and the target variable. <br>
<br>
The importance of creating a disjoint split between the training and testing sets lies in its capacity to provide an objective evaluation of the model's performance. A disjoint split guarantees that the testing set is a collection of observations that the model has not previously encountered during the training phase. This prevents the model from simply memorizing specific data pointsâ€”a phenomenon known as overfittingâ€”and instead encourages the development of generalization capabilities. By evaluating the model on the testing set, we gain insight into how well it can apply its learned patterns to new, unseen data, which is a robust indicator of its potential efficacy in real-world applications. <br>
<br>
</p>

<b>Training Data</b> <br>
<img class="proImg" src="projectImages/trainData.png" alt="Final data">

<b>Testing data</b> <br>
<img class="proImg" src="projectImages/testData.png" alt="Final data">
</section>

<header>
<h4 class="h4 article-title">Code</h4>
</header>

<section class="about-text">
<p>
The implementation of TabNet, along with sample data and code, is available at the provided link. This resource encompasses comprehensive details for both implementations, offering a practical perspective on applying TabNet.
</p>


<h6 class="h6"><a href="https://colab.research.google.com/drive/1UxIDuTZmPuWljqjAIR_DN_mf8cnUxZvT?usp=sharing" target="_blank">Click here to see the code for TabNet </a></h6>


</section>

          <header>
            <h4 class="h4 article-title">Results</h4>
          </header>
  
         
          <section class="about-text">
            <p>
              The performance of the TabNet model was meticulously evaluated using a comprehensive set of metrics, including accuracy, precision, recall, F1-score, and AUC-ROC. Additionally, training and validation loss/accuracy trends and the confusion matrix were analyzed to gain deeper insights into the modelâ€™s behavior and its classification capabilities.              
              <br>
              <b>Training and Validation Trends:</b> <br>
              
              <b>Accuracy</b> <br>
              <img class="proImg" src="projectImages/VaTN.png" alt="Training and Validation Accuracy">
              
              <b>Loss</b> <br>
              <img class="proImg" src="projectImages/TlTN.png" alt="Training and Validation Loss">
              Over 50 epochs, the TabNet model demonstrated remarkable improvements in both training and validation metrics, solidifying its effectiveness:    <br>
              - Training Accuracy progressively increased as the loss decreased, reflecting the modelâ€™s ability to learn complex relationships and optimize parameters efficiently.              <br>
              - Validation Accuracy showed consistent growth, culminating in an impressive 96.35% during the training phase and 96.78% on the test dataset. This trend indicates that the model generalized well to unseen data without overfitting.
              <br>
              - Training Loss decreased from an initial value of 0.5424 to 0.1050 by the final epoch, showcasing TabNetâ€™s ability to converge effectively while capturing meaningful feature interactions.
              <br>
              - Validation Loss followed a similar trajectory, reducing significantly as the epochs progressed, further reinforcing the modelâ€™s robustness.
              <br>
              The overall trend in the losses and accuracies validates that TabNet was able to balance memorization and generalization effectively, leading to high-quality predictions across the dataset. 
              <br>
              
              <b>Confusion Matrix Analysis:</b>
              
              <img class="proImg" src="projectImages/CmTN.png" alt="Confusion Matrix">
            

              The confusion matrix reveals valuable insights into the classification performance of TabNet: <br>
              - True Positives (1,227) and True Negatives (1,241) indicate the modelâ€™s exceptional ability to correctly classify instances from both positive and negative classes.
              <br>
              - False Positives (34) and False Negatives (48) were minimal, leading to high precision and recall values.
              <br>
              The balanced distribution of classification errors highlights the modelâ€™s capability to handle imbalanced data while ensuring both precision and recall are optimized, resulting in a robust F1-score.                 <br>
              <br>
          
              <b>Evaluation Metrics: </b> <br>
              TabNet achieved outstanding results on the test dataset:
              <br>
              - Test Accuracy: 96.78%, confirming the modelâ€™s exceptional ability to make reliable predictions on unseen data.<br>
              - Precision: 97.30%, demonstrating its capacity to minimize false positives, an essential feature for critical applications. <br>
              - Recall: 96.23%, reflecting the modelâ€™s proficiency in correctly identifying positive instances. <br>
              - F1-Score: 96.77%, providing a balanced measure of precision and recall. <br>
              - ROC AUC: 99.45%, signifying TabNetâ€™s excellent performance in distinguishing between positive and negative classes. <br>
              <br>
              The results underscore TabNetâ€™s remarkable ability to excel in tabular data classification tasks. Its unique architecture, leveraging sequential attention mechanisms and feature sparsity, enabled it to achieve a high level of predictive accuracy while maintaining interpretability. The low error rates, coupled with exceptional evaluation metrics, validate its applicability to real-world scenarios where accuracy and robustness are paramount.
              <br>
              
          </section>


          <header>
            <h4 class="h4 article-title">Conclusion</h4>
          </header>
          
          <section class="about-text">
            <p>
              
              The TabNet model demonstrated exceptional capabilities in handling the complexities of tabular data, delivering high predictive accuracy, robust generalization, and insightful interpretability. By leveraging its unique sequential attention mechanism and sparsity-inducing regularization, TabNet effectively captured intricate feature interactions while maintaining a high level of transparency in its decision-making process. This balance of performance and interpretability makes it a standout model in the realm of tabular data processing.              <br>
              <br>
              Through rigorous evaluation, TabNet achieved a test accuracy of 96.78%, an F1-score of 96.77%, and an outstanding AUC-ROC of 99.45%, underscoring its ability to make reliable and discriminative predictions. The steady decrease in training and validation losses, coupled with the consistently high validation accuracy, illustrates the model's capacity to optimize efficiently without overfitting. The confusion matrix further highlights the model's precision and recall, indicating its reliability in real-world applications where classification errors can have significant consequences.              <br>
              <br>
              TabNet's performance in this project reaffirms its status as a cutting-edge solution for tabular data problems. Its ability to combine interpretability, efficiency, and predictive power positions it as a valuable tool for tackling a wide array of practical challenges. These findings underscore TabNetâ€™s suitability for deployment in production systems, where both high accuracy and actionable insights are essential for driving impactful outcomes.
              <br>
              <br>
          
            </p>
                  
            </section>  
         
    </li>

          </ul> 

        </section>

      </article>





      <!--
        - #Certifications
      -->

      <!-- <article class="blog" data-page="certifications">

        <header>
          <h2 class="h2 article-title">Certifications</h2>
        </header>


      </article> -->





      <!--
        - #CONTACT
      -->

      <article class="contact" data-page="conclusion">

        <header>
          <h2 class="h2 article-title">Conclusion</h2>
        </header>
  
          <section class="about-text">
            <p>
             

The study of customer churn in the banking sector represents a profound exploration into the nuances of customer behavior and their dynamic relationships with financial institutions. As data is analyzed in depth, it becomes evident that the key to customer retention transcends mere transactional interactions; it is deeply rooted in personalized engagement and systematic nurturing of customer satisfaction. This project not only highlights the importance of addressing immediate customer needs but also emphasizes the necessity to anticipate and adapt to evolving customer expectations. Advanced predictive analytics reveal patterns that are critical for developing strategies aimed at reducing churn.

            </p>

            <p>
             
Analysis shows that understanding customer churn involves recognizing and respecting each customer's unique journey with their bank. This approach illuminates the importance of adopting a more relational strategy in banking practices, where every customer interaction is an opportunity to reaffirm trust and loyalty. The findings advocate for a shift towards proactive customer engagement, where feedback is seamlessly integrated into service improvement. This paradigm shift encourages banks to create interactions that are not just efficient but are also empathetically aligned with the diverse needs of customers, thereby enhancing satisfaction and loyalty.
            </p>
            
            <p>
            
Furthermore, the project underscores the critical role of sophisticated data-driven techniques in shaping strategies that effectively mitigate churn. By leveraging detailed data analyses and predictive modeling, financial institutions can gain invaluable insights into customer preferences and behavior patterns. This knowledge enables banks to tailor their services and communication in a way that resonates deeply with their customers, making each interaction more meaningful and likely to contribute to sustained engagement.
            </p>
  
            <p>
            This project has set forth a comprehensive framework for understanding and addressing customer churn through a blend of analytical rigor and strategic foresight. The insights derived from the analysis advocate for a transformation in how banks interact with customers, highlighting the necessity for a proactive, relationship-focused banking environment. As strategies continue to evolve, the path to reducing customer churn and enhancing loyalty lies in aligning with customer expectations and fostering a culture that values long-term relationships over short-term gains.
            </p>

            <p>
              The project's findings highlight a critical shift towards a more relational approach in banking, where every interaction is an opportunity to affirm the customerâ€™s trust. It advocates for a proactive engagement strategy, emphasizing the necessity of integrating customer feedback into service offerings. The essence of reducing churn, as revealed, lies in creating banking experiences that are not only efficient but also deeply resonant with customersâ€™ varied needs.
            </p>
            
<p>

  Ultimately, the sophisticated methodologies employed in this study serve as a testament to the power of data science in transforming business strategies and operations. This project not only contributes to the academic and practical understanding of customer churn but also provides a strategic blueprint for banks seeking to excel in customer retention. By embracing the insights and strategies outlined, banks can ensure they remain at the forefront of customer engagement, ready to meet the challenges of an evolving financial landscape.
</p>

            
          </section>
            
          </section>

          <header>
            <h2 class="h3 article-title">Future Work</h2>
          </header>
  
          <section class="about-text">
            <p>
              Future research in the realm of banking customer churn should focus on leveraging advanced analytics and emerging technologies to enhance predictive models and retention strategies. By incorporating detailed behavioral data and real-time analytics through artificial intelligence and machine learning, financial institutions can gain deeper insights into customer behaviors and preferences. This approach allows for proactive identification of churn risks and the delivery of personalized interventions, significantly improving customer satisfaction and loyalty. Additionally, analyzing customer sentiment from social media and feedback channels offers a nuanced understanding of customer experiences, enriching the data used to predict churn.
            </p>

            <p>
              Exploring the effects of financial wellness programs on customer retention presents another promising avenue. Such programs could deepen customer engagement and loyalty by aligning banking services more closely with individual financial goals. Moreover, considering the impact of broader industry trends, including regulatory changes and technological advancements, will be crucial in adapting retention strategies to the evolving banking landscape. 
            </p>

            <p>
              In essence, the future work should aim to blend sophisticated data analysis with a keen understanding of customer needs, creating a more dynamic and responsive banking experience. This balanced approach promises not only to mitigate churn but also to foster a banking environment where customer loyalty and satisfaction are paramount, ensuring long-term success in a competitive digital marketplace.
            </p>
  
            
          </section>

      </article>

    </div>

  </main>






  <!--
    - custom js link
  -->
  <script src="./assets/js/script.js"></script>

  <!--
    - ionicon link
  -->
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>

</body>

</html>
